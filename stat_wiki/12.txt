In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data.  The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance.\n\n\n== Approaches ==\n\n\n=== Grid search ===\nThe traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set\nor evaluation on a held-out validation set.Since the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.\nFor example, a typical soft-margin SVM classifier equipped with an RBF kernel has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant C and a kernel hyperparameter \u03b3. Both parameters are continuous, so to perform grid search, one selects a finite set of "reasonable" values for each, say\n\n  \n    \n      \n        C\n        \u2208\n        {\n        10\n        ,\n        100\n        ,\n        1000\n        }\n      \n    \n    {\\displaystyle C\\in \\{10,100,1000\\}}\n  \n\n  \n    \n      \n        \u03b3\n        \u2208\n        {\n        0.1\n        ,\n        0.2\n        ,\n        0.5\n        ,\n        1.0\n        }\n      \n    \n    {\\displaystyle \\gamma \\in \\{0.1,0.2,0.5,1.0\\}}\n  Grid search then trains an SVM with each pair (C, \u03b3) in the Cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.\nGrid search suffers from the curse of dimensionality, but is often embarrassingly parallel because the hyperparameter settings it evaluates are typically independent of each other.\n\n\n=== Random search ===\nRandom Search replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. In this case, the optimization problem is said to have a low intrinsic dimensionality. Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample.\n\n\n=== Bayesian optimization ===\n\nBayesian optimization is a global optimization method for noisy black-box functions.  Applied to hyperparameter optimization, Bayesian optimization builds a probabilistic model of the function mapping from hyperparameter values to the objective evaluated on a validation set. By iteratively evaluating a promising hyperparameter configuration based on the current model, and then updating it, Bayesian optimization, aims to gather observations revealing as much information as possible about this function and, in particular, the location of the optimum. It tries to balance exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters expected close to the optimum). In practice, Bayesian optimization has been shown to obtain better results in fewer evaluations compared to grid search and random search, due to the ability to reason about the quality of experiments before they are run.\n\n\n=== Gradient-based optimization ===\nFor specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression.A different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using  automatic differentiation. \n\n\n=== Evolutionary optimization ===\n\nEvolutionary optimization is a methodology for the global optimization of noisy black-box functions. In hyperparameter optimization, evolutionary optimization uses evolutionary algorithms to search the space of hyperparameters for a given algorithm. Evolutionary hyperparameter optimization follows a process inspired by the biological concept of evolution:\n\nCreate an initial population of random solutions (i.e., randomly generate tuples of hyperparameters, typically 100+)\nEvaluate the hyperparameters tuples and acquire their fitness function (e.g., 10-fold cross-validation accuracy of the machine learning algorithm with those hyperparameters)\nRank the hyperparameter tuples by their relative fitness\nReplace the worst-performing hyperparameter tuples with new hyperparameter tuples generated through crossover and mutation\nRepeat steps 2-4 until satisfactory algorithm performance is reached or algorithm performance is no longer improvingEvolutionary optimization has been used in hyperparameter optimization for statistical machine learning algorithms, automated machine learning, deep neural network architecture search, as well as training of the weights in deep neural networks.\n\n\n=== Population-based ===\nPopulation Based Training (PBT) learns both hyperparameter values and network weights. Multiple learning processes operate independently, using different hyperparameters. As with evolutionary methods, poorly performing models are iteratively replaced with models that adopt modified hyperparameter values and weights based on the better performers. This replacement model warm starting is the primary differentiator between PBT and other evolutionary methods. PBT thus allows the hyperparameters to evolve and eliminates the need for manual hypertuning. The process makes no assumptions regarding model architecture, loss functions or training procedures.\n\n\n=== Early Stopping-based ===\nA class of early stopping-based hyperparameter optimization algorithms is purpose built for large search spaces of continuous and discrete hyperparameters, particularly when the computational cost to evaluate the performance of a set of hyperparameters is high. The prototypical early stopping hyperparameter optimization algorithm is Successive Halving (SHA), which begins as a random search but periodically prunes low-performing models, thereby focusing computational resources on more promising models.  Asynchronous Successive Halving (ASHA) further improves upon SHA\u2019s resource utilization profile by removing the need to synchronously evaluate and prune low-performing models. Hyperband is a higher level early stopping-based algorithm that invokes SHA or ASHA multiple times with varying levels of pruning aggressiveness, in order to be more widely applicable and with fewer required inputs.\n\n\n=== Others ===\nRBF and spectral approaches have also been developed.\n\n\n== Open-source software ==\n\n\n=== Grid search ===\nDetermined, a DL Training Platform includes grid search for PyTorch and TensorFlow (Keras and Estimator) models.\nH2O AutoML provides grid search over algorithms in the H2O open source machine learning library.\nKatib is a Kubernetes-native system that includes grid search.\nscikit-learn is a Python package that includes grid search.\nTalos includes grid search for Keras.\nTune is a Python library for distributed hyperparameter tuning and supports grid search.\n\n\n=== Random search ===\nDetermined is a DL Training Platform that supports random search for PyTorch and TensorFlow (Keras and Estimator) models.\nhyperopt, also via hyperas and hyperopt-sklearn, are Python packages which include random search.\nKatib is a Kubernetes-native system that includes random search.\nscikit-learn is a Python package which includes random search.\nTalos includes a customizable random search for Keras.\nTune is a Python library for distributed hyperparameter tuning and supports random search over arbitrary parameter distributions.\n\n\n=== Bayesian ===\nAuto-sklearn is a Bayesian hyperparameter optimization layer on top of scikit-learn.\nAx is a Python-based experimentation platform that supports Bayesian optimization and bandit optimization as exploration strategies.\nBOCS is a Matlab package which uses semidefinite programming for minimizing a black-box function over discrete inputs. A Python 3 implementation is also included.\nHpBandSter is a Python package which combines Bayesian optimization with bandit-based methods.\nKatib is a Kubernetes-native system which includes bayesian optimization.\nmlrMBO, also with mlr, is an R package for model-based/Bayesian optimization of black-box functions.\noptuna is a Python package for black box optimization, compatible with arbitrary functions that need to be optimized.\nscikit-optimize is a Python package or sequential model-based optimization with a scipy.optimize interface.\nSMAC SMAC is a Python/Java library implementing Bayesian optimization.\ntuneRanger is an R package for tuning random forests using model-based optimization.\n\n\n=== Gradient-based optimization ===\nFAR-HO is a Python package containing Tensorflow implementations and wrappers for gradient-based hyperparamteter optimization with forward and reverse mode algorithmic differentiation.\nXGBoost is an open-source software library that provides a gradient boosting framework for C++, Java, Python, R, and Julia.\n\n\n=== Evolutionary ===\ndeap is a Python framework for general evolutionary computation which is flexible and integrates with parallelization packages like scoop and pyspark, and other Python frameworks like sklearn via sklearn-deap.\nDetermined is a DL Training Platform that supports PBT for optimizing PyTorch and TensorFlow (Keras and Estimator) models.\ndevol is a Python package that performs Deep Neural Network architecture search using genetic programming.\nnevergrad is a Python package which includes Differential_evolution, Evolution_strategy, Bayesian_optimization, population control methods for the noisy case and Particle_swarm_optimization.\nTune is a Python library for distributed hyperparameter tuning and leverages nevergrad for evolutionary algorithm support.\n\n\n=== Early Stopping ===\nDetermined is a DL Training Platform that supports Hyperband for PyTorch and TensorFlow (Keras and Estimator) models.\nKatib is a Kubernetes-native system that includes hyperband.\n\n\n=== Other ===\nDetermined is a DL Training Platform that supports random, grid, PBT, Hyperband and NAS approaches to hyperparameter optimization for PyTorch and TensorFlow (Keras and Estimator) models.\ndlib is a C++ package with a Python API which has a parameter-free optimizer based on LIPO and trust region optimizers working in tandem.\nHarmonica is a Python package for spectral hyperparameter optimization.\nhyperopt, also via hyperas and hyperopt-sklearn, are Python packages which include Tree of Parzen Estimators based distributed hyperparameter optimization.\nKatib is a Kubernetes-native system which includes grid, random search, bayesian optimization, hyperband, and NAS based on reinforcement learning.\nnevergrad is a Python package for gradient-free optimization using techniques such as differential evolution, sequential quadratic programming, fastGA, covariance matrix adaptation, population control methods, and particle swarm optimization.\nNeural Network Intelligence (NNI) is a Python package which includes hyperparameter tuning for neural networks in local and distributed environments. Its techniques include TPE, random, anneal, evolution, SMAC, batch, grid, and hyperband.\nparameter-sherpa is a similar Python package which includes several techniques grid search, Bayesian and genetic Optimization\nphotonai is a high level Python API for designing and optimizing machine learning pipelines based on grid, random search and bayesian optimization.\npycma is a Python implementation of Covariance Matrix Adaptation Evolution Strategy.\nrbfopt is a Python package that uses a radial basis function model\nTune is a Python library for hyperparameter tuning execution and integrates with/scales many existing hyperparameter optimization libraries such as hyperopt, nevergrad, and scikit-optimize.\n\n\n== Commercial services ==\nAmazon Sagemaker uses Gaussian processes to tune hyperparameters.\nBigML OptiML supports mixed search domains\nGoogle HyperTune supports mixed search domains\nIndie Solver supports multiobjective, multifidelity and constraint optimization\nMind Foundry OPTaaS supports mixed search domains, multiobjective, constraints, parallel optimization and surrogate models.\nSigOpt supports mixed search domains, multiobjective, multisolution, multifidelity, constraint (linear and black-box), and parallel optimization.\n\n\n== See also ==\nAutomated machine learning\nNeural architecture search\nMeta-optimization\nModel selection\nSelf-tuning\nXGBoost\n\n\n== References ==