In statistics, Bayesian linear regression is an approach to linear regression in which the statistical analysis is undertaken within the context of Bayesian inference. When the regression model has errors that have a normal distribution, and if a particular form of prior distribution is assumed, explicit results are available for the posterior probability distributions of the model's parameters.\n\n\n== Model setup ==\nConsider a standard linear regression problem, in which for \n  \n    \n      \n        i\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n      \n    \n    {\\displaystyle i=1,\\ldots ,n}\n   we specify the mean of the conditional distribution of \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n   given a \n  \n    \n      \n        k\n        \xd7\n        1\n      \n    \n    {\\displaystyle k\\times 1}\n   predictor vector \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}}\n  :\n\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        \n          \n            x\n          \n          \n            i\n          \n          \n            \n              T\n            \n          \n        \n        \n          \u03b2\n        \n        +\n        \n          \u03b5\n          \n            i\n          \n        \n        ,\n      \n    \n    {\\displaystyle y_{i}=\\mathbf {x} _{i}^{\\rm {T}}{\\boldsymbol {\\beta }}+\\varepsilon _{i},}\n  where \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n   is a \n  \n    \n      \n        k\n        \xd7\n        1\n      \n    \n    {\\displaystyle k\\times 1}\n   vector, and the \n  \n    \n      \n        \n          \u03b5\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{i}}\n   are independent and identically normally distributed random variables:\n\n  \n    \n      \n        \n          \u03b5\n          \n            i\n          \n        \n        \u223c\n        N\n        (\n        0\n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle \\varepsilon _{i}\\sim N(0,\\sigma ^{2}).}\n  This corresponds to the following likelihood function:\n\n  \n    \n      \n        \u03c1\n        (\n        \n          y\n        \n        \u2223\n        \n          X\n        \n        ,\n        \n          \u03b2\n        \n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        \u221d\n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          )\n          \n            \u2212\n            n\n            \n              /\n            \n            2\n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                1\n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n            (\n            \n              y\n            \n            \u2212\n            \n              X\n            \n            \n              \u03b2\n            \n            \n              )\n              \n                \n                  T\n                \n              \n            \n            (\n            \n              y\n            \n            \u2212\n            \n              X\n            \n            \n              \u03b2\n            \n            )\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\rho (\\mathbf {y} \\mid \\mathbf {X} ,{\\boldsymbol {\\beta }},\\sigma ^{2})\\propto (\\sigma ^{2})^{-n/2}\\exp \\left(-{\\frac {1}{2\\sigma ^{2}}}(\\mathbf {y} -\\mathbf {X} {\\boldsymbol {\\beta }})^{\\rm {T}}(\\mathbf {y} -\\mathbf {X} {\\boldsymbol {\\beta }})\\right).}\n  The ordinary least squares solution is used to estimate the coefficient vector using the Moore\u2013Penrose pseudoinverse:\n\n  \n    \n      \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        =\n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        \n          )\n          \n            \u2212\n            1\n          \n        \n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          y\n        \n      \n    \n    {\\displaystyle {\\hat {\\boldsymbol {\\beta }}}=(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} )^{-1}\\mathbf {X} ^{\\rm {T}}\\mathbf {y} }\n  where \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\displaystyle \\mathbf {X} }\n   is the \n  \n    \n      \n        n\n        \xd7\n        k\n      \n    \n    {\\displaystyle n\\times k}\n   design matrix, each row of which is a predictor vector \n  \n    \n      \n        \n          \n            x\n          \n          \n            i\n          \n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{i}^{\\rm {T}}}\n  ; and \n  \n    \n      \n        \n          y\n        \n      \n    \n    {\\displaystyle \\mathbf {y} }\n   is the column \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  -vector \n  \n    \n      \n        [\n        \n          y\n          \n            1\n          \n        \n        \n        \u22ef\n        \n        \n          y\n          \n            n\n          \n        \n        \n          ]\n          \n            \n              T\n            \n          \n        \n      \n    \n    {\\displaystyle [y_{1}\\;\\cdots \\;y_{n}]^{\\rm {T}}}\n  .\nThis is a frequentist approach, and it assumes that there are enough measurements to say something meaningful about \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n  .  In the Bayesian approach, the data are supplemented with additional information in the form of a prior probability distribution. The prior belief about the parameters is combined with the data's likelihood function according to Bayes theorem to yield the posterior belief about the parameters \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n   and \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  . The prior can take different functional forms depending on the domain and the information that is available a priori.\n\n\n== With conjugate priors ==\n\n\n=== Conjugate prior distribution ===\nFor an arbitrary prior distribution, there may be no analytical solution for the posterior distribution. In this section, we will consider a so-called conjugate prior for which the posterior distribution can be derived analytically.\nA prior \n  \n    \n      \n        \u03c1\n        (\n        \n          \u03b2\n        \n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\rho ({\\boldsymbol {\\beta }},\\sigma ^{2})}\n   is conjugate to this likelihood function if it has the same functional form with respect to \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n   and \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  . Since the log-likelihood is quadratic in \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n  , the log-likelihood is re-written such that the likelihood becomes normal in \n  \n    \n      \n        (\n        \n          \u03b2\n        \n        \u2212\n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle ({\\boldsymbol {\\beta }}-{\\hat {\\boldsymbol {\\beta }}})}\n  .  Write\n\n  \n    \n      \n        (\n        \n          y\n        \n        \u2212\n        \n          X\n        \n        \n          \u03b2\n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n        (\n        \n          y\n        \n        \u2212\n        \n          X\n        \n        \n          \u03b2\n        \n        )\n        =\n        (\n        \n          y\n        \n        \u2212\n        \n          X\n        \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n        (\n        \n          y\n        \n        \u2212\n        \n          X\n        \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        )\n        +\n        (\n        \n          \u03b2\n        \n        \u2212\n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        )\n        (\n        \n          \u03b2\n        \n        \u2212\n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle (\\mathbf {y} -\\mathbf {X} {\\boldsymbol {\\beta }})^{\\rm {T}}(\\mathbf {y} -\\mathbf {X} {\\boldsymbol {\\beta }})=(\\mathbf {y} -\\mathbf {X} {\\hat {\\boldsymbol {\\beta }}})^{\\rm {T}}(\\mathbf {y} -\\mathbf {X} {\\hat {\\boldsymbol {\\beta }}})+({\\boldsymbol {\\beta }}-{\\hat {\\boldsymbol {\\beta }}})^{\\rm {T}}(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} )({\\boldsymbol {\\beta }}-{\\hat {\\boldsymbol {\\beta }}}).}\n  The likelihood is now re-written as\n\n  \n    \n      \n        \u03c1\n        (\n        \n          y\n        \n        \n          |\n        \n        \n          X\n        \n        ,\n        \n          \u03b2\n        \n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        \u221d\n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          )\n          \n            \u2212\n            \n              \n                v\n                2\n              \n            \n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                \n                  v\n                  \n                    s\n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    \n                      \u03c3\n                    \n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n          )\n        \n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          )\n          \n            \u2212\n            \n              \n                \n                  n\n                  \u2212\n                  v\n                \n                2\n              \n            \n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                1\n                \n                  2\n                  \n                    \n                      \u03c3\n                    \n                    \n                      2\n                    \n                  \n                \n              \n            \n            (\n            \n              \u03b2\n            \n            \u2212\n            \n              \n                \n                  \u03b2\n                  ^\n                \n              \n            \n            \n              )\n              \n                \n                  T\n                \n              \n            \n            (\n            \n              \n                X\n              \n              \n                \n                  T\n                \n              \n            \n            \n              X\n            \n            )\n            (\n            \n              \u03b2\n            \n            \u2212\n            \n              \n                \n                  \u03b2\n                  ^\n                \n              \n            \n            )\n          \n          )\n        \n        ,\n      \n    \n    {\\displaystyle \\rho (\\mathbf {y} |\\mathbf {X} ,{\\boldsymbol {\\beta }},\\sigma ^{2})\\propto (\\sigma ^{2})^{-{\\frac {v}{2}}}\\exp \\left(-{\\frac {vs^{2}}{2{\\sigma }^{2}}}\\right)(\\sigma ^{2})^{-{\\frac {n-v}{2}}}\\exp \\left(-{\\frac {1}{2{\\sigma }^{2}}}({\\boldsymbol {\\beta }}-{\\hat {\\boldsymbol {\\beta }}})^{\\rm {T}}(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} )({\\boldsymbol {\\beta }}-{\\hat {\\boldsymbol {\\beta }}})\\right),}\n  where\n\n  \n    \n      \n        v\n        \n          s\n          \n            2\n          \n        \n        =\n        (\n        \n          y\n        \n        \u2212\n        \n          X\n        \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n        (\n        \n          y\n        \n        \u2212\n        \n          X\n        \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        )\n        \n        \n           and \n        \n        \n        v\n        =\n        n\n        \u2212\n        k\n        ,\n      \n    \n    {\\displaystyle vs^{2}=(\\mathbf {y} -\\mathbf {X} {\\hat {\\boldsymbol {\\beta }}})^{\\rm {T}}(\\mathbf {y} -\\mathbf {X} {\\hat {\\boldsymbol {\\beta }}})\\quad {\\text{ and }}\\quad v=n-k,}\n  where \n  \n    \n      \n        k\n      \n    \n    {\\displaystyle k}\n   is the number of regression coefficients.\nThis suggests a form for the prior:\n\n  \n    \n      \n        \u03c1\n        (\n        \n          \u03b2\n        \n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        =\n        \u03c1\n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        \u03c1\n        (\n        \n          \u03b2\n        \n        \u2223\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\rho ({\\boldsymbol {\\beta }},\\sigma ^{2})=\\rho (\\sigma ^{2})\\rho ({\\boldsymbol {\\beta }}\\mid \\sigma ^{2}),}\n  where \n  \n    \n      \n        \u03c1\n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\rho (\\sigma ^{2})}\n   is an inverse-gamma distribution\n\n  \n    \n      \n        \u03c1\n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        \u221d\n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          )\n          \n            \u2212\n            \n              \n                \n                  v\n                  \n                    0\n                  \n                \n                2\n              \n            \n            \u2212\n            1\n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                \n                  \n                    v\n                    \n                      0\n                    \n                  \n                  \n                    s\n                    \n                      0\n                    \n                    \n                      2\n                    \n                  \n                \n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\rho (\\sigma ^{2})\\propto (\\sigma ^{2})^{-{\\frac {v_{0}}{2}}-1}\\exp \\left(-{\\frac {v_{0}s_{0}^{2}}{2\\sigma ^{2}}}\\right).}\n  In the notation introduced  in the inverse-gamma distribution article, this is the density of an \n  \n    \n      \n        \n          Inv-Gamma\n        \n        (\n        \n          a\n          \n            0\n          \n        \n        ,\n        \n          b\n          \n            0\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\text{Inv-Gamma}}(a_{0},b_{0})}\n   distribution with \n  \n    \n      \n        \n          a\n          \n            0\n          \n        \n        =\n        \n          \n            \n              \n                v\n                \n                  0\n                \n              \n              2\n            \n          \n        \n      \n    \n    {\\displaystyle a_{0}={\\tfrac {v_{0}}{2}}}\n   and \n  \n    \n      \n        \n          b\n          \n            0\n          \n        \n        =\n        \n          \n            \n              1\n              2\n            \n          \n        \n        \n          v\n          \n            0\n          \n        \n        \n          s\n          \n            0\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle b_{0}={\\tfrac {1}{2}}v_{0}s_{0}^{2}}\n   with \n  \n    \n      \n        \n          v\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle v_{0}}\n   and \n  \n    \n      \n        \n          s\n          \n            0\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle s_{0}^{2}}\n   as the prior values of \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   and \n  \n    \n      \n        \n          s\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle s^{2}}\n  , respectively.  Equivalently, it can also be described as a scaled inverse chi-squared distribution, \n  \n    \n      \n        \n          Scale-inv-\n        \n        \n          \u03c7\n          \n            2\n          \n        \n        (\n        \n          v\n          \n            0\n          \n        \n        ,\n        \n          s\n          \n            0\n          \n          \n            2\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\text{Scale-inv-}}\\chi ^{2}(v_{0},s_{0}^{2}).}\n  \nFurther the conditional prior density \n  \n    \n      \n        \u03c1\n        (\n        \n          \u03b2\n        \n        \n          |\n        \n        \n          \u03c3\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle \\rho ({\\boldsymbol {\\beta }}|\\sigma ^{2})}\n   is a normal distribution,\n\n  \n    \n      \n        \u03c1\n        (\n        \n          \u03b2\n        \n        \u2223\n        \n          \u03c3\n          \n            2\n          \n        \n        )\n        \u221d\n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          )\n          \n            \u2212\n            k\n            \n              /\n            \n            2\n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                1\n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n            (\n            \n              \u03b2\n            \n            \u2212\n            \n              \n                \u03bc\n              \n              \n                0\n              \n            \n            \n              )\n              \n                \n                  T\n                \n              \n            \n            \n              \n                \u039b\n              \n              \n                0\n              \n            \n            (\n            \n              \u03b2\n            \n            \u2212\n            \n              \n                \u03bc\n              \n              \n                0\n              \n            \n            )\n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\rho ({\\boldsymbol {\\beta }}\\mid \\sigma ^{2})\\propto (\\sigma ^{2})^{-k/2}\\exp \\left(-{\\frac {1}{2\\sigma ^{2}}}({\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{0})^{\\rm {T}}\\mathbf {\\Lambda } _{0}({\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{0})\\right).}\n  In the notation of the normal distribution, the conditional prior distribution is \n  \n    \n      \n        \n          \n            N\n          \n        \n        \n          (\n          \n            \n              \n                \u03bc\n              \n              \n                0\n              \n            \n            ,\n            \n              \u03c3\n              \n                2\n              \n            \n            \n              \n                \u039b\n              \n              \n                0\n              \n              \n                \u2212\n                1\n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle {\\mathcal {N}}\\left({\\boldsymbol {\\mu }}_{0},\\sigma ^{2}\\mathbf {\\Lambda } _{0}^{-1}\\right).}\n  \n\n\n=== Posterior distribution ===\nWith the prior now specified, the posterior distribution can be expressed as\n\n  \n    \n      \n        \n          \n            \n              \n                \u03c1\n                (\n                \n                  \u03b2\n                \n                ,\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n                \u2223\n                \n                  y\n                \n                ,\n                \n                  X\n                \n                )\n              \n              \n                \n                \u221d\n                \u03c1\n                (\n                \n                  y\n                \n                \u2223\n                \n                  X\n                \n                ,\n                \n                  \u03b2\n                \n                ,\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n                )\n                \u03c1\n                (\n                \n                  \u03b2\n                \n                \u2223\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n                )\n                \u03c1\n                (\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n                )\n              \n            \n            \n              \n              \n                \n                \u221d\n                (\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n                \n                  )\n                  \n                    \u2212\n                    n\n                    \n                      /\n                    \n                    2\n                  \n                \n                exp\n                \u2061\n                \n                  (\n                  \n                    \u2212\n                    \n                      \n                        1\n                        \n                          2\n                          \n                            \n                              \u03c3\n                            \n                            \n                              2\n                            \n                          \n                        \n                      \n                    \n                    (\n                    \n                      y\n                    \n                    \u2212\n                    \n                      X\n                    \n                    \n                      \u03b2\n                    \n                    \n                      )\n                      \n                        \n                          T\n                        \n                      \n                    \n                    (\n                    \n                      y\n                    \n                    \u2212\n                    \n                      X\n                    \n                    \n                      \u03b2\n                    \n                    )\n                  \n                  )\n                \n                (\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n                \n                  )\n                  \n                    \u2212\n                    k\n                    \n                      /\n                    \n                    2\n                  \n                \n                exp\n                \u2061\n                \n                  (\n                  \n                    \u2212\n                    \n                      \n                        1\n                        \n                          2\n                          \n                            \u03c3\n                            \n                              2\n                            \n                          \n                        \n                      \n                    \n                    (\n                    \n                      \u03b2\n                    \n                    \u2212\n                    \n                      \n                        \u03bc\n                      \n                      \n                        0\n                      \n                    \n                    \n                      )\n                      \n                        \n                          T\n                        \n                      \n                    \n                    \n                      \n                        \u039b\n                      \n                      \n                        0\n                      \n                    \n                    (\n                    \n                      \u03b2\n                    \n                    \u2212\n                    \n                      \n                        \u03bc\n                      \n                      \n                        0\n                      \n                    \n                    )\n                  \n                  )\n                \n                (\n                \n                  \u03c3\n                  \n                    2\n                  \n                \n                \n                  )\n                  \n                    \u2212\n                    (\n                    \n                      a\n                      \n                        0\n                      \n                    \n                    +\n                    1\n                    )\n                  \n                \n                exp\n                \u2061\n                \n                  (\n                  \n                    \u2212\n                    \n                      \n                        \n                          b\n                          \n                            0\n                          \n                        \n                        \n                          \u03c3\n                          \n                            2\n                          \n                        \n                      \n                    \n                  \n                  )\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\rho ({\\boldsymbol {\\beta }},\\sigma ^{2}\\mid \\mathbf {y} ,\\mathbf {X} )&\\propto \\rho (\\mathbf {y} \\mid \\mathbf {X} ,{\\boldsymbol {\\beta }},\\sigma ^{2})\\rho ({\\boldsymbol {\\beta }}\\mid \\sigma ^{2})\\rho (\\sigma ^{2})\\\\&\\propto (\\sigma ^{2})^{-n/2}\\exp \\left(-{\\frac {1}{2{\\sigma }^{2}}}(\\mathbf {y} -\\mathbf {X} {\\boldsymbol {\\beta }})^{\\rm {T}}(\\mathbf {y} -\\mathbf {X} {\\boldsymbol {\\beta }})\\right)(\\sigma ^{2})^{-k/2}\\exp \\left(-{\\frac {1}{2\\sigma ^{2}}}({\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{0})^{\\rm {T}}{\\boldsymbol {\\Lambda }}_{0}({\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{0})\\right)(\\sigma ^{2})^{-(a_{0}+1)}\\exp \\left(-{\\frac {b_{0}}{\\sigma ^{2}}}\\right)\\end{aligned}}}\n  With some re-arrangement, the posterior can be re-written so that the posterior mean \n  \n    \n      \n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}_{n}}\n   of the parameter vector \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n   can be expressed in terms of the least squares estimator \n  \n    \n      \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\boldsymbol {\\beta }}}}\n   and the prior mean \n  \n    \n      \n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}_{0}}\n  , with the strength of the prior indicated by the prior precision matrix \n  \n    \n      \n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\Lambda }}_{0}}\n  \n\n  \n    \n      \n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n        =\n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        +\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        \n          )\n          \n            \u2212\n            1\n          \n        \n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        +\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}_{n}=(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} +{\\boldsymbol {\\Lambda }}_{0})^{-1}(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} {\\hat {\\boldsymbol {\\beta }}}+{\\boldsymbol {\\Lambda }}_{0}{\\boldsymbol {\\mu }}_{0}).}\n  To justify that \n  \n    \n      \n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}_{n}}\n   is indeed the posterior mean, the quadratic terms in the exponential can be re-arranged as a quadratic form in \n  \n    \n      \n        \n          \u03b2\n        \n        \u2212\n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{n}}\n  .\n\n  \n    \n      \n        (\n        \n          y\n        \n        \u2212\n        \n          X\n        \n        \n          \u03b2\n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n        (\n        \n          y\n        \n        \u2212\n        \n          X\n        \n        \n          \u03b2\n        \n        )\n        +\n        (\n        \n          \u03b2\n        \n        \u2212\n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        (\n        \n          \u03b2\n        \n        \u2212\n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n        )\n        =\n        (\n        \n          \u03b2\n        \n        \u2212\n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n        \n          )\n          \n            \n              T\n            \n          \n        \n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        +\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        )\n        (\n        \n          \u03b2\n        \n        \u2212\n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n        )\n        +\n        \n          \n            y\n          \n          \n            \n              T\n            \n          \n        \n        \n          y\n        \n        \u2212\n        \n          \n            \u03bc\n          \n          \n            n\n          \n          \n            \n              T\n            \n          \n        \n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        +\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        )\n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n        +\n        \n          \n            \u03bc\n          \n          \n            0\n          \n          \n            \n              T\n            \n          \n        \n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n        .\n      \n    \n    {\\displaystyle (\\mathbf {y} -\\mathbf {X} {\\boldsymbol {\\beta }})^{\\rm {T}}(\\mathbf {y} -\\mathbf {X} {\\boldsymbol {\\beta }})+({\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{0})^{\\rm {T}}{\\boldsymbol {\\Lambda }}_{0}({\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{0})=({\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{n})^{\\rm {T}}(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} +{\\boldsymbol {\\Lambda }}_{0})({\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{n})+\\mathbf {y} ^{\\rm {T}}\\mathbf {y} -{\\boldsymbol {\\mu }}_{n}^{\\rm {T}}(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} +{\\boldsymbol {\\Lambda }}_{0}){\\boldsymbol {\\mu }}_{n}+{\\boldsymbol {\\mu }}_{0}^{\\rm {T}}{\\boldsymbol {\\Lambda }}_{0}{\\boldsymbol {\\mu }}_{0}.}\n  Now the posterior can be expressed as a normal distribution times an inverse-gamma distribution:\n\n  \n    \n      \n        \u03c1\n        (\n        \n          \u03b2\n        \n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        \u2223\n        \n          y\n        \n        ,\n        \n          X\n        \n        )\n        \u221d\n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          )\n          \n            \u2212\n            k\n            \n              /\n            \n            2\n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                1\n                \n                  2\n                  \n                    \n                      \u03c3\n                    \n                    \n                      2\n                    \n                  \n                \n              \n            \n            (\n            \n              \u03b2\n            \n            \u2212\n            \n              \n                \u03bc\n              \n              \n                n\n              \n            \n            \n              )\n              \n                \n                  T\n                \n              \n            \n            (\n            \n              \n                X\n              \n              \n                \n                  T\n                \n              \n            \n            \n              X\n            \n            +\n            \n              \n                \u039b\n              \n              \n                0\n              \n            \n            )\n            (\n            \n              \u03b2\n            \n            \u2212\n            \n              \n                \u03bc\n              \n              \n                n\n              \n            \n            )\n          \n          )\n        \n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        \n          )\n          \n            \u2212\n            \n              \n                \n                  n\n                  +\n                  2\n                  \n                    a\n                    \n                      0\n                    \n                  \n                \n                2\n              \n            \n            \u2212\n            1\n          \n        \n        exp\n        \u2061\n        \n          (\n          \n            \u2212\n            \n              \n                \n                  2\n                  \n                    b\n                    \n                      0\n                    \n                  \n                  +\n                  \n                    \n                      y\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                  \n                    y\n                  \n                  \u2212\n                  \n                    \n                      \u03bc\n                    \n                    \n                      n\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                  (\n                  \n                    \n                      X\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                  \n                    X\n                  \n                  +\n                  \n                    \n                      \u039b\n                    \n                    \n                      0\n                    \n                  \n                  )\n                  \n                    \n                      \u03bc\n                    \n                    \n                      n\n                    \n                  \n                  +\n                  \n                    \n                      \u03bc\n                    \n                    \n                      0\n                    \n                    \n                      \n                        T\n                      \n                    \n                  \n                  \n                    \n                      \u039b\n                    \n                    \n                      0\n                    \n                  \n                  \n                    \n                      \u03bc\n                    \n                    \n                      0\n                    \n                  \n                \n                \n                  2\n                  \n                    \u03c3\n                    \n                      2\n                    \n                  \n                \n              \n            \n          \n          )\n        \n        .\n      \n    \n    {\\displaystyle \\rho ({\\boldsymbol {\\beta }},\\sigma ^{2}\\mid \\mathbf {y} ,\\mathbf {X} )\\propto (\\sigma ^{2})^{-k/2}\\exp \\left(-{\\frac {1}{2{\\sigma }^{2}}}({\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{n})^{\\rm {T}}(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} +\\mathbf {\\Lambda } _{0})({\\boldsymbol {\\beta }}-{\\boldsymbol {\\mu }}_{n})\\right)(\\sigma ^{2})^{-{\\frac {n+2a_{0}}{2}}-1}\\exp \\left(-{\\frac {2b_{0}+\\mathbf {y} ^{\\rm {T}}\\mathbf {y} -{\\boldsymbol {\\mu }}_{n}^{\\rm {T}}(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} +{\\boldsymbol {\\Lambda }}_{0}){\\boldsymbol {\\mu }}_{n}+{\\boldsymbol {\\mu }}_{0}^{\\rm {T}}{\\boldsymbol {\\Lambda }}_{0}{\\boldsymbol {\\mu }}_{0}}{2\\sigma ^{2}}}\\right).}\n  Therefore, the posterior distribution can be parametrized as follows.\n\n  \n    \n      \n        \u03c1\n        (\n        \n          \u03b2\n        \n        ,\n        \n          \u03c3\n          \n            2\n          \n        \n        \u2223\n        \n          y\n        \n        ,\n        \n          X\n        \n        )\n        \u221d\n        \u03c1\n        (\n        \n          \u03b2\n        \n        \u2223\n        \n          \u03c3\n          \n            2\n          \n        \n        ,\n        \n          y\n        \n        ,\n        \n          X\n        \n        )\n        \u03c1\n        (\n        \n          \u03c3\n          \n            2\n          \n        \n        \u2223\n        \n          y\n        \n        ,\n        \n          X\n        \n        )\n        ,\n      \n    \n    {\\displaystyle \\rho ({\\boldsymbol {\\beta }},\\sigma ^{2}\\mid \\mathbf {y} ,\\mathbf {X} )\\propto \\rho ({\\boldsymbol {\\beta }}\\mid \\sigma ^{2},\\mathbf {y} ,\\mathbf {X} )\\rho (\\sigma ^{2}\\mid \\mathbf {y} ,\\mathbf {X} ),}\n  where the two factors correspond to the densities of \n  \n    \n      \n        \n          \n            N\n          \n        \n        \n          (\n          \n            \n              \n                \u03bc\n              \n              \n                n\n              \n            \n            ,\n            \n              \u03c3\n              \n                2\n              \n            \n            \n              \n                \u039b\n              \n              \n                n\n              \n              \n                \u2212\n                1\n              \n            \n          \n          )\n        \n        \n      \n    \n    {\\displaystyle {\\mathcal {N}}\\left({\\boldsymbol {\\mu }}_{n},\\sigma ^{2}{\\boldsymbol {\\Lambda }}_{n}^{-1}\\right)\\,}\n   and \n  \n    \n      \n        \n          Inv-Gamma\n        \n        \n          (\n          \n            \n              a\n              \n                n\n              \n            \n            ,\n            \n              b\n              \n                n\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle {\\text{Inv-Gamma}}\\left(a_{n},b_{n}\\right)}\n   distributions, with the parameters of these given by\n\n  \n    \n      \n        \n          \n            \u039b\n          \n          \n            n\n          \n        \n        =\n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        +\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        )\n        ,\n        \n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n        =\n        (\n        \n          \n            \u039b\n          \n          \n            n\n          \n        \n        \n          )\n          \n            \u2212\n            1\n          \n        \n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        +\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle {\\boldsymbol {\\Lambda }}_{n}=(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} +\\mathbf {\\Lambda } _{0}),\\quad {\\boldsymbol {\\mu }}_{n}=({\\boldsymbol {\\Lambda }}_{n})^{-1}(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} {\\hat {\\boldsymbol {\\beta }}}+{\\boldsymbol {\\Lambda }}_{0}{\\boldsymbol {\\mu }}_{0}),}\n  \n\n  \n    \n      \n        \n          a\n          \n            n\n          \n        \n        =\n        \n          a\n          \n            0\n          \n        \n        +\n        \n          \n            n\n            2\n          \n        \n        ,\n        \n        \n          b\n          \n            n\n          \n        \n        =\n        \n          b\n          \n            0\n          \n        \n        +\n        \n          \n            1\n            2\n          \n        \n        (\n        \n          \n            y\n          \n          \n            \n              T\n            \n          \n        \n        \n          y\n        \n        +\n        \n          \n            \u03bc\n          \n          \n            0\n          \n          \n            \n              T\n            \n          \n        \n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n        \u2212\n        \n          \n            \u03bc\n          \n          \n            n\n          \n          \n            \n              T\n            \n          \n        \n        \n          \n            \u039b\n          \n          \n            n\n          \n        \n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle a_{n}=a_{0}+{\\frac {n}{2}},\\qquad b_{n}=b_{0}+{\\frac {1}{2}}(\\mathbf {y} ^{\\rm {T}}\\mathbf {y} +{\\boldsymbol {\\mu }}_{0}^{\\rm {T}}{\\boldsymbol {\\Lambda }}_{0}{\\boldsymbol {\\mu }}_{0}-{\\boldsymbol {\\mu }}_{n}^{\\rm {T}}{\\boldsymbol {\\Lambda }}_{n}{\\boldsymbol {\\mu }}_{n}).}\n  This can be interpreted as Bayesian learning where the parameters are updated according to the following equations.\n\n  \n    \n      \n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n        =\n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        +\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        \n          )\n          \n            \u2212\n            1\n          \n        \n        (\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n        +\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        \n          \n            \n              \u03b2\n              ^\n            \n          \n        \n        )\n        =\n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        +\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        \n          )\n          \n            \u2212\n            1\n          \n        \n        (\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n        +\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          y\n        \n        )\n        ,\n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}_{n}=(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} +{\\boldsymbol {\\Lambda }}_{0})^{-1}({\\boldsymbol {\\Lambda }}_{0}{\\boldsymbol {\\mu }}_{0}+\\mathbf {X} ^{\\rm {T}}\\mathbf {X} {\\hat {\\boldsymbol {\\beta }}})=(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} +{\\boldsymbol {\\Lambda }}_{0})^{-1}({\\boldsymbol {\\Lambda }}_{0}{\\boldsymbol {\\mu }}_{0}+\\mathbf {X} ^{\\rm {T}}\\mathbf {y} ),}\n  \n\n  \n    \n      \n        \n          \n            \u039b\n          \n          \n            n\n          \n        \n        =\n        (\n        \n          \n            X\n          \n          \n            \n              T\n            \n          \n        \n        \n          X\n        \n        +\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        )\n        ,\n      \n    \n    {\\displaystyle {\\boldsymbol {\\Lambda }}_{n}=(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} +{\\boldsymbol {\\Lambda }}_{0}),}\n  \n\n  \n    \n      \n        \n          a\n          \n            n\n          \n        \n        =\n        \n          a\n          \n            0\n          \n        \n        +\n        \n          \n            n\n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle a_{n}=a_{0}+{\\frac {n}{2}},}\n  \n\n  \n    \n      \n        \n          b\n          \n            n\n          \n        \n        =\n        \n          b\n          \n            0\n          \n        \n        +\n        \n          \n            1\n            2\n          \n        \n        (\n        \n          \n            y\n          \n          \n            \n              T\n            \n          \n        \n        \n          y\n        \n        +\n        \n          \n            \u03bc\n          \n          \n            0\n          \n          \n            \n              T\n            \n          \n        \n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n        \u2212\n        \n          \n            \u03bc\n          \n          \n            n\n          \n          \n            \n              T\n            \n          \n        \n        \n          \n            \u039b\n          \n          \n            n\n          \n        \n        \n          \n            \u03bc\n          \n          \n            n\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle b_{n}=b_{0}+{\\frac {1}{2}}(\\mathbf {y} ^{\\rm {T}}\\mathbf {y} +{\\boldsymbol {\\mu }}_{0}^{\\rm {T}}{\\boldsymbol {\\Lambda }}_{0}{\\boldsymbol {\\mu }}_{0}-{\\boldsymbol {\\mu }}_{n}^{\\rm {T}}{\\boldsymbol {\\Lambda }}_{n}{\\boldsymbol {\\mu }}_{n}).}\n  \n\n\n=== Model evidence ===\nThe model evidence \n  \n    \n      \n        p\n        (\n        \n          y\n        \n        \u2223\n        m\n        )\n      \n    \n    {\\displaystyle p(\\mathbf {y} \\mid m)}\n   is the probability of the data given the model \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  . It is also known as the marginal likelihood, and as the prior predictive density. Here, the model is defined by the likelihood function \n  \n    \n      \n        p\n        (\n        \n          y\n        \n        \u2223\n        \n          X\n        \n        ,\n        \n          \u03b2\n        \n        ,\n        \u03c3\n        )\n      \n    \n    {\\displaystyle p(\\mathbf {y} \\mid \\mathbf {X} ,{\\boldsymbol {\\beta }},\\sigma )}\n   and the prior distribution on the parameters, i.e. \n  \n    \n      \n        p\n        (\n        \n          \u03b2\n        \n        ,\n        \u03c3\n        )\n      \n    \n    {\\displaystyle p({\\boldsymbol {\\beta }},\\sigma )}\n  . The model evidence captures in a single number how well such a model explains the observations. The model evidence of the Bayesian linear regression model presented in this section can be used to compare competing linear models by Bayesian model comparison. These models may differ in the number and values of the predictor variables as well as in their priors on the model parameters. Model complexity is already taken into account by the model evidence, because it marginalizes out the parameters by integrating \n  \n    \n      \n        p\n        (\n        \n          y\n        \n        ,\n        \n          \u03b2\n        \n        ,\n        \u03c3\n        \u2223\n        \n          X\n        \n        )\n      \n    \n    {\\displaystyle p(\\mathbf {y} ,{\\boldsymbol {\\beta }},\\sigma \\mid \\mathbf {X} )}\n   over all possible values of \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n   and \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  .\n\n  \n    \n      \n        p\n        (\n        \n          y\n        \n        \n          |\n        \n        m\n        )\n        =\n        \u222b\n        p\n        (\n        \n          y\n        \n        \u2223\n        \n          X\n        \n        ,\n        \n          \u03b2\n        \n        ,\n        \u03c3\n        )\n        \n        p\n        (\n        \n          \u03b2\n        \n        ,\n        \u03c3\n        )\n        \n        d\n        \n          \u03b2\n        \n        \n        d\n        \u03c3\n      \n    \n    {\\displaystyle p(\\mathbf {y} |m)=\\int p(\\mathbf {y} \\mid \\mathbf {X} ,{\\boldsymbol {\\beta }},\\sigma )\\,p({\\boldsymbol {\\beta }},\\sigma )\\,d{\\boldsymbol {\\beta }}\\,d\\sigma }\n  This integral can be computed analytically and the solution is given in the following equation.\n\n  \n    \n      \n        p\n        (\n        \n          y\n        \n        \u2223\n        m\n        )\n        =\n        \n          \n            1\n            \n              (\n              2\n              \u03c0\n              \n                )\n                \n                  n\n                  \n                    /\n                  \n                  2\n                \n              \n            \n          \n        \n        \n          \n            \n              \n                det\n                (\n                \n                  \n                    \u039b\n                  \n                  \n                    0\n                  \n                \n                )\n              \n              \n                det\n                (\n                \n                  \n                    \u039b\n                  \n                  \n                    n\n                  \n                \n                )\n              \n            \n          \n        \n        \u22c5\n        \n          \n            \n              b\n              \n                0\n              \n              \n                \n                  a\n                  \n                    0\n                  \n                \n              \n            \n            \n              b\n              \n                n\n              \n              \n                \n                  a\n                  \n                    n\n                  \n                \n              \n            \n          \n        \n        \u22c5\n        \n          \n            \n              \u0393\n              (\n              \n                a\n                \n                  n\n                \n              \n              )\n            \n            \n              \u0393\n              (\n              \n                a\n                \n                  0\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle p(\\mathbf {y} \\mid m)={\\frac {1}{(2\\pi )^{n/2}}}{\\sqrt {\\frac {\\det({\\boldsymbol {\\Lambda }}_{0})}{\\det({\\boldsymbol {\\Lambda }}_{n})}}}\\cdot {\\frac {b_{0}^{a_{0}}}{b_{n}^{a_{n}}}}\\cdot {\\frac {\\Gamma (a_{n})}{\\Gamma (a_{0})}}}\n  Here \n  \n    \n      \n        \u0393\n      \n    \n    {\\displaystyle \\Gamma }\n   denotes the gamma function. Because we have chosen a conjugate prior, the marginal likelihood can also be easily computed by evaluating the following equality for arbitrary values of \n  \n    \n      \n        \n          \u03b2\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\beta }}}\n   and \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n  .\n\n  \n    \n      \n        p\n        (\n        \n          y\n        \n        \u2223\n        m\n        )\n        =\n        \n          \n            \n              p\n              (\n              \n                \u03b2\n              \n              ,\n              \u03c3\n              \n                |\n              \n              m\n              )\n              \n              p\n              (\n              \n                y\n              \n              \u2223\n              \n                X\n              \n              ,\n              \n                \u03b2\n              \n              ,\n              \u03c3\n              ,\n              m\n              )\n            \n            \n              p\n              (\n              \n                \u03b2\n              \n              ,\n              \u03c3\n              \u2223\n              \n                y\n              \n              ,\n              \n                X\n              \n              ,\n              m\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle p(\\mathbf {y} \\mid m)={\\frac {p({\\boldsymbol {\\beta }},\\sigma |m)\\,p(\\mathbf {y} \\mid \\mathbf {X} ,{\\boldsymbol {\\beta }},\\sigma ,m)}{p({\\boldsymbol {\\beta }},\\sigma \\mid \\mathbf {y} ,\\mathbf {X} ,m)}}}\n  Note that this equation is nothing but a re-arrangement of Bayes theorem. Inserting the formulas for the prior, the likelihood, and the posterior and simplifying the resulting expression leads to the analytic expression given above.\n\n\n== Other cases ==\nIn general, it may be impossible or impractical to derive the posterior distribution analytically. However, it is possible to approximate the posterior by an approximate Bayesian inference method such as Monte Carlo sampling or variational Bayes.\nThe special case \n  \n    \n      \n        \n          \n            \u03bc\n          \n          \n            0\n          \n        \n        =\n        0\n        ,\n        \n          \n            \u039b\n          \n          \n            0\n          \n        \n        =\n        c\n        \n          I\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}_{0}=0,\\mathbf {\\Lambda } _{0}=c\\mathbf {I} }\n   is called ridge regression.\nA similar analysis can be performed for the general case of the multivariate regression and part of this provides for Bayesian estimation of covariance matrices: see Bayesian multivariate linear regression.\n\n\n== See also ==\nBayes linear statistics\nRegularized least squares\nTikhonov regularization\nSpike and slab variable selection\nBayesian interpretation of kernel regularization\n\n\n== Notes ==\n\n\n== References ==\nBox, G. E. P.; Tiao, G. C. (1973). Bayesian Inference in Statistical Analysis. Wiley. ISBN 0-471-57428-7.\nCarlin, Bradley P.; Louis, Thomas A. (2008). Bayesian Methods for Data Analysis, Third Edition. Boca Raton, FL: Chapman and Hall/CRC. ISBN 1-58488-697-8.\nFahrmeir, L.; Kneib, T.; Lang, S. (2009). Regression. Modelle, Methoden und Anwendungen (Second ed.). Heidelberg: Springer. doi:10.1007/978-3-642-01837-4. ISBN 978-3-642-01836-7.\nFornalski K.W.; Parzych G.; Pylak M.; Satu\u0142a D.; Dobrzy\u0144ski L. (2010). "Application of Bayesian reasoning and the Maximum Entropy Method to some reconstruction problems". Acta Physica Polonica A. 117 (6): 892\u2013899. doi:10.12693/APhysPolA.117.892.\nFornalski, Krzysztof W. (2015). "Applications of the robust Bayesian regression analysis". International Journal of Society Systems Science. 7 (4): 314\u2013333. doi:10.1504/IJSSS.2015.073223.\nGelman, Andrew; Carlin, John B.; Stern, Hal S.; Rubin, Donald B. (2003). Bayesian Data Analysis, Second Edition. Boca Raton, FL: Chapman and Hall/CRC. ISBN 1-58488-388-X.\nGoldstein, Michael; Wooff, David (2007). Bayes Linear Statistics, Theory & Methods. Wiley. ISBN 978-0-470-01562-9.\nMinka, Thomas P. (2001) Bayesian Linear Regression, Microsoft research web page\nRossi, Peter E.; Allenby, Greg M.; McCulloch, Robert (2006). Bayesian Statistics and Marketing. John Wiley & Sons. ISBN 0470863676.\nO'Hagan, Anthony (1994). Bayesian Inference. Kendall's Advanced Theory of Statistics. 2B (First ed.). Halsted. ISBN 0-340-52922-9.\nSivia, D.S.; Skilling, J. (2006). Data Analysis - A Bayesian Tutorial (Second ed.). Oxford University Press.\nWalter, Gero; Augustin, Thomas (2009). "Bayesian Linear Regression\u2014Different Conjugate Models and Their (In)Sensitivity to Prior-Data Conflict" (PDF). Technical Report Number 069, Department of Statistics, University of Munich.\n\n\n== External links ==\nBayesian estimation of linear models (R programming wikibook). Bayesian linear regression as implemented in R.