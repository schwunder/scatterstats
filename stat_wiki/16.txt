Empirical Bayes methods are procedures for  statistical inference in which the prior distribution is estimated from the data.  This approach stands in contrast to standard Bayesian methods, for which the prior distribution is fixed before any data are observed.  Despite this difference in perspective, empirical Bayes may be viewed as an approximation to a fully Bayesian treatment of a hierarchical model wherein the parameters at the highest level of the hierarchy are set to their most likely values, instead of being integrated out.  Empirical Bayes, also known as maximum marginal likelihood, represents one approach for setting hyperparameters.\n\n\n== Introduction ==\nEmpirical Bayes methods can be seen as an approximation to a fully Bayesian treatment of a hierarchical Bayes model.\nIn, for example, a two-stage hierarchical Bayes model, observed data \n  \n    \n      \n        y\n        =\n        {\n        \n          y\n          \n            1\n          \n        \n        ,\n        \n          y\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          y\n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle y=\\{y_{1},y_{2},\\dots ,y_{n}\\}}\n   are assumed to be generated from an unobserved set of parameters \n  \n    \n      \n        \u03b8\n        =\n        {\n        \n          \u03b8\n          \n            1\n          \n        \n        ,\n        \n          \u03b8\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \u03b8\n          \n            n\n          \n        \n        }\n      \n    \n    {\\displaystyle \\theta =\\{\\theta _{1},\\theta _{2},\\dots ,\\theta _{n}\\}}\n   according to a probability distribution \n  \n    \n      \n        p\n        (\n        y\n        \u2223\n        \u03b8\n        )\n        \n      \n    \n    {\\displaystyle p(y\\mid \\theta )\\,}\n  .  In turn, the parameters \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   can be considered samples drawn from a population characterised by hyperparameters \n  \n    \n      \n        \u03b7\n        \n      \n    \n    {\\displaystyle \\eta \\,}\n   according to a probability distribution \n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        \u03b7\n        )\n        \n      \n    \n    {\\displaystyle p(\\theta \\mid \\eta )\\,}\n  .  In the hierarchical Bayes model, though not in the empirical Bayes approximation, the hyperparameters \n  \n    \n      \n        \u03b7\n        \n      \n    \n    {\\displaystyle \\eta \\,}\n   are considered to be drawn from an unparameterized distribution \n  \n    \n      \n        p\n        (\n        \u03b7\n        )\n        \n      \n    \n    {\\displaystyle p(\\eta )\\,}\n  .\nInformation about a particular quantity of interest \n  \n    \n      \n        \n          \u03b8\n          \n            i\n          \n        \n        \n      \n    \n    {\\displaystyle \\theta _{i}\\;}\n   therefore comes not only from the properties of those data which directly depend on it, but also from the properties of the population of parameters \n  \n    \n      \n        \u03b8\n        \n      \n    \n    {\\displaystyle \\theta \\;}\n   as a whole, inferred from the data as a whole, summarised by the hyperparameters \n  \n    \n      \n        \u03b7\n        \n      \n    \n    {\\displaystyle \\eta \\;}\n  .\nUsing Bayes' theorem,\n\n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        =\n        \n          \n            \n              p\n              (\n              y\n              \u2223\n              \u03b8\n              )\n              p\n              (\n              \u03b8\n              )\n            \n            \n              p\n              (\n              y\n              )\n            \n          \n        \n        =\n        \n          \n            \n              p\n              (\n              y\n              \u2223\n              \u03b8\n              )\n            \n            \n              p\n              (\n              y\n              )\n            \n          \n        \n        \u222b\n        p\n        (\n        \u03b8\n        \u2223\n        \u03b7\n        )\n        p\n        (\n        \u03b7\n        )\n        \n        d\n        \u03b7\n        \n        .\n      \n    \n    {\\displaystyle p(\\theta \\mid y)={\\frac {p(y\\mid \\theta )p(\\theta )}{p(y)}}={\\frac {p(y\\mid \\theta )}{p(y)}}\\int p(\\theta \\mid \\eta )p(\\eta )\\,d\\eta \\,.}\n  In general, this integral will not be tractable analytically or symbolically and must be evaluated by numerical methods. Stochastic (random) or deterministic approximations may be used.  Example stochastic methods are Markov Chain Monte Carlo and Monte Carlo sampling.  Deterministic approximations are discussed in quadrature.\nAlternatively, the expression can be written as\n\n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        =\n        \u222b\n        p\n        (\n        \u03b8\n        \u2223\n        \u03b7\n        ,\n        y\n        )\n        p\n        (\n        \u03b7\n        \u2223\n        y\n        )\n        \n        d\n        \u03b7\n        =\n        \u222b\n        \n          \n            \n              p\n              (\n              y\n              \u2223\n              \u03b8\n              )\n              p\n              (\n              \u03b8\n              \u2223\n              \u03b7\n              )\n            \n            \n              p\n              (\n              y\n              \u2223\n              \u03b7\n              )\n            \n          \n        \n        p\n        (\n        \u03b7\n        \u2223\n        y\n        )\n        \n        d\n        \u03b7\n        \n        ,\n      \n    \n    {\\displaystyle p(\\theta \\mid y)=\\int p(\\theta \\mid \\eta ,y)p(\\eta \\mid y)\\;d\\eta =\\int {\\frac {p(y\\mid \\theta )p(\\theta \\mid \\eta )}{p(y\\mid \\eta )}}p(\\eta \\mid y)\\;d\\eta \\,,}\n  and the term in the integral can in turn be expressed as\n\n  \n    \n      \n        p\n        (\n        \u03b7\n        \u2223\n        y\n        )\n        =\n        \u222b\n        p\n        (\n        \u03b7\n        \u2223\n        \u03b8\n        )\n        p\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        \n        d\n        \u03b8\n        .\n      \n    \n    {\\displaystyle p(\\eta \\mid y)=\\int p(\\eta \\mid \\theta )p(\\theta \\mid y)\\;d\\theta .}\n  These suggest an iterative scheme, qualitatively similar in structure to a Gibbs sampler, to evolve successively improved approximations to \n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\theta \\mid y)\\;}\n   and \n  \n    \n      \n        p\n        (\n        \u03b7\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\eta \\mid y)\\;}\n  .  First, calculate an initial approximation to \n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\theta \\mid y)\\;}\n   ignoring the \n  \n    \n      \n        \u03b7\n      \n    \n    {\\displaystyle \\eta }\n   dependence completely; then calculate an approximation to \n  \n    \n      \n        p\n        (\n        \u03b7\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\eta \\mid y)\\;}\n   based upon the initial approximate distribution of \n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\theta \\mid y)\\;}\n  ; then use this \n  \n    \n      \n        p\n        (\n        \u03b7\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\eta \\mid y)\\;}\n   to update the approximation for \n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\theta \\mid y)\\;}\n  ; then update \n  \n    \n      \n        p\n        (\n        \u03b7\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\eta \\mid y)\\;}\n  ; and so on.\nWhen the true distribution \n  \n    \n      \n        p\n        (\n        \u03b7\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\eta \\mid y)\\;}\n   is sharply peaked, the integral determining \n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\theta \\mid y)\\;}\n   may be not much changed by replacing the probability distribution over \n  \n    \n      \n        \u03b7\n        \n      \n    \n    {\\displaystyle \\eta \\;}\n   with a point estimate \n  \n    \n      \n        \n          \u03b7\n          \n            \u2217\n          \n        \n        \n      \n    \n    {\\displaystyle \\eta ^{*}\\;}\n   representing the distribution's peak (or, alternatively, its mean),\n\n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        \u2243\n        \n          \n            \n              p\n              (\n              y\n              \u2223\n              \u03b8\n              )\n              \n              p\n              (\n              \u03b8\n              \u2223\n              \n                \u03b7\n                \n                  \u2217\n                \n              \n              )\n            \n            \n              p\n              (\n              y\n              \u2223\n              \n                \u03b7\n                \n                  \u2217\n                \n              \n              )\n            \n          \n        \n        \n        .\n      \n    \n    {\\displaystyle p(\\theta \\mid y)\\simeq {\\frac {p(y\\mid \\theta )\\;p(\\theta \\mid \\eta ^{*})}{p(y\\mid \\eta ^{*})}}\\,.}\n  With this approximation, the above iterative scheme becomes the EM algorithm.\nThe term "Empirical Bayes" can cover a wide variety of methods, but most can be regarded as an early truncation of either the above scheme or something quite like it.  Point estimates, rather than the whole distribution, are typically used for the parameter(s) \n  \n    \n      \n        \u03b7\n        \n      \n    \n    {\\displaystyle \\eta \\;}\n  . The estimates for \n  \n    \n      \n        \n          \u03b7\n          \n            \u2217\n          \n        \n        \n      \n    \n    {\\displaystyle \\eta ^{*}\\;}\n   are typically made from the first approximation to \n  \n    \n      \n        p\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        \n      \n    \n    {\\displaystyle p(\\theta \\mid y)\\;}\n   without subsequent refinement. These estimates for \n  \n    \n      \n        \n          \u03b7\n          \n            \u2217\n          \n        \n        \n      \n    \n    {\\displaystyle \\eta ^{*}\\;}\n   are usually made without considering an appropriate prior distribution for \n  \n    \n      \n        \u03b7\n      \n    \n    {\\displaystyle \\eta }\n  .\n\n\n== Point estimation ==\n\n\n=== Robbins method : non-parametric empirical Bayes (NPEB) ===\nRobbins considered a case of sampling from a mixed distribution, where probability for each \n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle y_{i}}\n   (conditional on \n  \n    \n      \n        \n          \u03b8\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\theta _{i}}\n  ) is specified by  a Poisson distribution,\n\n  \n    \n      \n        p\n        (\n        \n          y\n          \n            i\n          \n        \n        \u2223\n        \n          \u03b8\n          \n            i\n          \n        \n        )\n        =\n        \n          \n            \n              \n                \n                  \n                    \u03b8\n                    \n                      i\n                    \n                  \n                \n                \n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n              \n              \n                e\n                \n                  \u2212\n                  \n                    \u03b8\n                    \n                      i\n                    \n                  \n                \n              \n            \n            \n              \n                \n                  y\n                  \n                    i\n                  \n                \n              \n              !\n            \n          \n        \n      \n    \n    {\\displaystyle p(y_{i}\\mid \\theta _{i})={{\\theta _{i}}^{y_{i}}e^{-\\theta _{i}} \\over {y_{i}}!}}\n  while the prior on \u03b8 is unspecified except that it is also i.i.d. from an unknown distribution, with cumulative distribution function \n  \n    \n      \n        G\n        (\n        \u03b8\n        )\n      \n    \n    {\\displaystyle G(\\theta )}\n  .  Compound sampling arises in a variety of statistical estimation problems, such as accident rates and clinical trials.  We simply seek a point prediction of \n  \n    \n      \n        \n          \u03b8\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle \\theta _{i}}\n   given all the observed data.  Because the prior is unspecified, we seek to do this without knowledge of G.Under squared error loss (SEL), the conditional expectation E(\u03b8i | Yi = yi) is a reasonable quantity to use for prediction.  For the Poisson compound sampling model, this quantity is\n\n  \n    \n      \n        E\n        \u2061\n        (\n        \n          \u03b8\n          \n            i\n          \n        \n        \u2223\n        \n          y\n          \n            i\n          \n        \n        )\n        =\n        \n          \n            \n              \u222b\n              (\n              \n                \u03b8\n                \n                  \n                    y\n                    \n                      i\n                    \n                  \n                  +\n                  1\n                \n              \n              \n                e\n                \n                  \u2212\n                  \u03b8\n                \n              \n              \n                /\n              \n              \n                \n                  y\n                  \n                    i\n                  \n                \n              \n              !\n              )\n              \n              d\n              G\n              (\n              \u03b8\n              )\n            \n            \n              \n                \u222b\n                (\n                \n                  \u03b8\n                  \n                    \n                      y\n                      \n                        i\n                      \n                    \n                  \n                \n                \n                  e\n                  \n                    \u2212\n                    \u03b8\n                  \n                \n                \n                  /\n                \n                \n                  \n                    y\n                    \n                      i\n                    \n                  \n                \n                !\n                )\n                \n                d\n                G\n                (\n                \u03b8\n              \n              )\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\operatorname {E} (\\theta _{i}\\mid y_{i})={\\int (\\theta ^{y_{i}+1}e^{-\\theta }/{y_{i}}!)\\,dG(\\theta ) \\over {\\int (\\theta ^{y_{i}}e^{-\\theta }/{y_{i}}!)\\,dG(\\theta })}.}\n  This can be simplified by multiplying the expression by \n  \n    \n      \n        (\n        \n          \n            y\n            \n              i\n            \n          \n        \n        +\n        1\n        )\n        \n          /\n        \n        (\n        \n          \n            y\n            \n              i\n            \n          \n        \n        +\n        1\n        )\n      \n    \n    {\\displaystyle ({y_{i}}+1)/({y_{i}}+1)}\n  , yielding\n\n  \n    \n      \n        E\n        \u2061\n        (\n        \n          \u03b8\n          \n            i\n          \n        \n        \u2223\n        \n          y\n          \n            i\n          \n        \n        )\n        =\n        \n          \n            \n              (\n              \n                y\n                \n                  i\n                \n              \n              +\n              1\n              )\n              \n                p\n                \n                  G\n                \n              \n              (\n              \n                y\n                \n                  i\n                \n              \n              +\n              1\n              )\n            \n            \n              \n                p\n                \n                  G\n                \n              \n              (\n              \n                y\n                \n                  i\n                \n              \n              )\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\operatorname {E} (\\theta _{i}\\mid y_{i})={{(y_{i}+1)p_{G}(y_{i}+1)} \\over {p_{G}(y_{i})}},}\n  where pG is the marginal distribution obtained by integrating out \u03b8 over G.\nTo take advantage of this, Robbins suggested estimating the marginals with their empirical frequencies, yielding the fully non-parametric estimate as:\n\n  \n    \n      \n        E\n        \u2061\n        (\n        \n          \u03b8\n          \n            i\n          \n        \n        \u2223\n        \n          y\n          \n            i\n          \n        \n        )\n        \u2248\n        (\n        \n          y\n          \n            i\n          \n        \n        +\n        1\n        )\n        \n          \n            \n              #\n              {\n              \n                Y\n                \n                  j\n                \n              \n              =\n              \n                y\n                \n                  i\n                \n              \n              +\n              1\n              }\n            \n            \n              #\n              {\n              \n                Y\n                \n                  j\n                \n              \n              =\n              \n                y\n                \n                  i\n                \n              \n              }\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\operatorname {E} (\\theta _{i}\\mid y_{i})\\approx (y_{i}+1){{\\#\\{Y_{j}=y_{i}+1\\}} \\over {\\#\\{Y_{j}=y_{i}\\}}},}\n  where \n  \n    \n      \n        #\n      \n    \n    {\\displaystyle \\#}\n   denotes "number of". (See also Good\u2013Turing frequency estimation.)\n\nExample \u2013 Accident ratesSuppose each customer of an insurance company has an "accident rate" \u0398 and is insured against accidents; the probability distribution of \u0398  is the underlying distribution, and is unknown.  The number of accidents suffered by each customer in a specified time period has a Poisson distribution with expected value equal to the particular customer's accident rate.  The actual number of accidents experienced by a customer is the observable quantity. A crude way to estimate the underlying probability distribution of the accident rate \u0398 is to estimate the proportion of members of the whole population suffering 0, 1, 2, 3, ... accidents during the specified time period as the corresponding proportion in the observed random sample.  Having done so, it is then desired to predict the accident rate of each customer in the sample.  As above, one may use the conditional expected value of the accident rate \u0398 given the observed number of accidents during the baseline period.  Thus, if a customer suffers six accidents during the baseline period, that customer's estimated accident rate is 7 \xd7 [the proportion of the sample who suffered 7 accidents] / [the proportion of the sample who suffered 6 accidents].  Note that if the proportion of people suffering k accidents is a decreasing function of k, the customer's predicted accident rate will often be lower than their observed number of accidents.\nThis shrinkage effect is typical of empirical Bayes analyses.\n\n\n=== Parametric empirical Bayes ===\nIf the likelihood and its prior take on simple parametric forms (such as 1- or 2-dimensional likelihood functions with simple conjugate priors), then the empirical Bayes problem  is only to estimate the marginal \n  \n    \n      \n        m\n        (\n        y\n        \u2223\n        \u03b7\n        )\n      \n    \n    {\\displaystyle m(y\\mid \\eta )}\n   and the hyperparameters \n  \n    \n      \n        \u03b7\n      \n    \n    {\\displaystyle \\eta }\n   using the complete set of empirical measurements.   For example, one common approach, called parametric empirical Bayes point estimation, is to approximate the marginal using the maximum likelihood estimate (MLE), or a Moments expansion, which allows one to express the hyperparameters \n  \n    \n      \n        \u03b7\n      \n    \n    {\\displaystyle \\eta }\n   in terms of the empirical mean and variance.  This simplified marginal allows one to plug in the empirical averages into a point estimate for the prior \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  .  The resulting equation for the prior \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   is greatly simplified, as shown below.\nThere are several common parametric empirical Bayes models, including the Poisson\u2013gamma model (below), the Beta-binomial model, the Gaussian\u2013Gaussian model, the Dirichlet-multinomial model, as well specific models for Bayesian linear regression (see below) and Bayesian multivariate linear regression. More advanced approaches include hierarchical Bayes models and Bayesian mixture models.\n\n\n==== Poisson\u2013gamma model ====\nFor example, in the example above, let the likelihood be a Poisson distribution, and let the prior now be specified by the conjugate prior, which is a gamma distribution (\n  \n    \n      \n        G\n        (\n        \u03b1\n        ,\n        \u03b2\n        )\n      \n    \n    {\\displaystyle G(\\alpha ,\\beta )}\n  ) (where \n  \n    \n      \n        \u03b7\n        =\n        (\n        \u03b1\n        ,\n        \u03b2\n        )\n      \n    \n    {\\displaystyle \\eta =(\\alpha ,\\beta )}\n  ):\n\n  \n    \n      \n        \u03c1\n        (\n        \u03b8\n        \u2223\n        \u03b1\n        ,\n        \u03b2\n        )\n        =\n        \n          \n            \n              \n                \u03b8\n                \n                  \u03b1\n                  \u2212\n                  1\n                \n              \n              \n              \n                e\n                \n                  \u2212\n                  \u03b8\n                  \n                    /\n                  \n                  \u03b2\n                \n              \n            \n            \n              \n                \u03b2\n                \n                  \u03b1\n                \n              \n              \u0393\n              (\n              \u03b1\n              )\n            \n          \n        \n         \n        \n          f\n          o\n          r\n        \n         \n        \u03b8\n        >\n        0\n        ,\n        \u03b1\n        >\n        0\n        ,\n        \u03b2\n        >\n        0\n        \n        \n        .\n      \n    \n    {\\displaystyle \\rho (\\theta \\mid \\alpha ,\\beta )={\\frac {\\theta ^{\\alpha -1}\\,e^{-\\theta /\\beta }}{\\beta ^{\\alpha }\\Gamma (\\alpha )}}\\ \\mathrm {for} \\ \\theta >0,\\alpha >0,\\beta >0\\,\\!.}\n  It is straightforward to show the posterior is also a gamma distribution.  Write\n\n  \n    \n      \n        \u03c1\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        \u221d\n        \u03c1\n        (\n        y\n        \u2223\n        \u03b8\n        )\n        \u03c1\n        (\n        \u03b8\n        \u2223\n        \u03b1\n        ,\n        \u03b2\n        )\n        ,\n      \n    \n    {\\displaystyle \\rho (\\theta \\mid y)\\propto \\rho (y\\mid \\theta )\\rho (\\theta \\mid \\alpha ,\\beta ),}\n  where the marginal distribution has been omitted since it does not depend explicitly on \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n  .\nExpanding terms which do depend on \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   gives the posterior as:\n\n  \n    \n      \n        \u03c1\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        \u221d\n        (\n        \n          \u03b8\n          \n            y\n          \n        \n        \n        \n          e\n          \n            \u2212\n            \u03b8\n          \n        \n        )\n        (\n        \n          \u03b8\n          \n            \u03b1\n            \u2212\n            1\n          \n        \n        \n        \n          e\n          \n            \u2212\n            \u03b8\n            \n              /\n            \n            \u03b2\n          \n        \n        )\n        =\n        \n          \u03b8\n          \n            y\n            +\n            \u03b1\n            \u2212\n            1\n          \n        \n        \n        \n          e\n          \n            \u2212\n            \u03b8\n            (\n            1\n            +\n            1\n            \n              /\n            \n            \u03b2\n            )\n          \n        \n        .\n      \n    \n    {\\displaystyle \\rho (\\theta \\mid y)\\propto (\\theta ^{y}\\,e^{-\\theta })(\\theta ^{\\alpha -1}\\,e^{-\\theta /\\beta })=\\theta ^{y+\\alpha -1}\\,e^{-\\theta (1+1/\\beta )}.}\n  So the posterior density is also a gamma distribution \n  \n    \n      \n        G\n        (\n        \n          \u03b1\n          \u2032\n        \n        ,\n        \n          \u03b2\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle G(\\alpha ',\\beta ')}\n  , where \n  \n    \n      \n        \n          \u03b1\n          \u2032\n        \n        =\n        y\n        +\n        \u03b1\n      \n    \n    {\\displaystyle \\alpha '=y+\\alpha }\n  , and \n  \n    \n      \n        \n          \u03b2\n          \u2032\n        \n        =\n        (\n        1\n        +\n        1\n        \n          /\n        \n        \u03b2\n        \n          )\n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle \\beta '=(1+1/\\beta )^{-1}}\n  .  Also notice that the marginal is simply the integral of the posterior over all \n  \n    \n      \n        \u0398\n      \n    \n    {\\displaystyle \\Theta }\n  , which turns out to be a negative binomial distribution.\nTo apply empirical Bayes, we will approximate the marginal using the maximum likelihood estimate (MLE). But since the posterior is a gamma distribution, the MLE of the marginal turns out to be just the mean of the posterior, which is the point estimate \n  \n    \n      \n        E\n        \u2061\n        (\n        \u03b8\n        \u2223\n        y\n        )\n      \n    \n    {\\displaystyle \\operatorname {E} (\\theta \\mid y)}\n   we need. Recalling that the mean \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   of a gamma distribution \n  \n    \n      \n        G\n        (\n        \n          \u03b1\n          \u2032\n        \n        ,\n        \n          \u03b2\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle G(\\alpha ',\\beta ')}\n   is simply \n  \n    \n      \n        \n          \u03b1\n          \u2032\n        \n        \n          \u03b2\n          \u2032\n        \n      \n    \n    {\\displaystyle \\alpha '\\beta '}\n  , we have\n\n  \n    \n      \n        E\n        \u2061\n        (\n        \u03b8\n        \u2223\n        y\n        )\n        =\n        \n          \u03b1\n          \u2032\n        \n        \n          \u03b2\n          \u2032\n        \n        =\n        \n          \n            \n              \n                \n                  \n                    y\n                    \xaf\n                  \n                \n              \n              +\n              \u03b1\n            \n            \n              1\n              +\n              1\n              \n                /\n              \n              \u03b2\n            \n          \n        \n        =\n        \n          \n            \u03b2\n            \n              1\n              +\n              \u03b2\n            \n          \n        \n        \n          \n            \n              y\n              \xaf\n            \n          \n        \n        +\n        \n          \n            1\n            \n              1\n              +\n              \u03b2\n            \n          \n        \n        (\n        \u03b1\n        \u03b2\n        )\n        .\n      \n    \n    {\\displaystyle \\operatorname {E} (\\theta \\mid y)=\\alpha '\\beta '={\\frac {{\\bar {y}}+\\alpha }{1+1/\\beta }}={\\frac {\\beta }{1+\\beta }}{\\bar {y}}+{\\frac {1}{1+\\beta }}(\\alpha \\beta ).}\n  To obtain the values of \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n   and \n  \n    \n      \n        \u03b2\n      \n    \n    {\\displaystyle \\beta }\n  , empirical Bayes prescribes estimating mean \n  \n    \n      \n        \u03b1\n        \u03b2\n      \n    \n    {\\displaystyle \\alpha \\beta }\n   and variance \n  \n    \n      \n        \u03b1\n        \n          \u03b2\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\alpha \\beta ^{2}}\n   using the complete set of empirical data.\nThe resulting point estimate \n  \n    \n      \n        E\n        \u2061\n        (\n        \u03b8\n        \u2223\n        y\n        )\n      \n    \n    {\\displaystyle \\operatorname {E} (\\theta \\mid y)}\n   is therefore like a weighted average of the sample mean \n  \n    \n      \n        \n          \n            \n              y\n              \xaf\n            \n          \n        \n      \n    \n    {\\displaystyle {\\bar {y}}}\n   and the prior mean \n  \n    \n      \n        \u03bc\n        =\n        \u03b1\n        \u03b2\n      \n    \n    {\\displaystyle \\mu =\\alpha \\beta }\n  .  This turns out to be a general feature of empirical Bayes; the point estimates for the prior (i.e. mean) will look like a weighted averages of the sample estimate and the prior estimate (likewise for estimates of the variance).\n\n\n== See also ==\nBayes estimator\nBest linear unbiased prediction\nMonty Hall problem\nRobbins lemma\nSpike-and-slab variable selection\n\n\n== References ==\n\n\n== Further reading ==\nPeter E. Rossi; Greg M. Allenby; Rob McCulloch (14 May 2012). Bayesian Statistics and Marketing. John Wiley & Sons. ISBN 978-0-470-86368-8.\nCasella, George (May 1985). "An Introduction to Empirical Bayes Data Analysis" (PDF). American Statistician. 39 (2): 83\u201387. doi:10.2307/2682801. JSTOR 2682801. MR 0789118.\nNikulin, Mikhail (1987). "Bernstein's regularity conditions in a problem of empirical Bayesian approach". Journal of Soviet Mathematics. 36 (5): 596\u2013600. doi:10.1007/BF01093293.\n\n\n== External links ==\nUse of empirical Bayes Method in estimating road safety (North America)\nEmpirical Bayes methods for missing data analysis\nUsing the Beta-Binomial distribution to assess performance of a biometric identification device\nA Hierarchical Naive Bayes Classifiers (for continuous and discrete variables).