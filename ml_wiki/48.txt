Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters. Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and approximate inference and learning methods are used.\n\n\n== Applications ==\nFor example, the problem of translating a natural language sentence into a syntactic representation such as a parse tree can be seen as a structured prediction problem in which the structured output domain is the set of all possible parse trees.\nStructured prediction is also used in a wide variety of application domains including bioinformatics, natural language processing, speech recognition, and computer vision. \n\n\n=== Example: sequence tagging ===\nSequence tagging is a class of problems prevalent in natural language processing, where input data are often sequences (e.g. sentences of text). The sequence tagging problem appears in several guises, e.g. part-of-speech tagging and named entity recognition. In POS tagging, for example, each word in a sequence must receive a "tag" (class label) that expresses its "type" of word:\n\nThe main challenge of this problem is to resolve ambiguity: the word "sentence" can also be a verb in English, and so can "tagged".\nWhile this problem can be solved by simply performing classification of individual tokens, that approach does not take into account the empirical fact that tags do not occur independently; instead, each tag displays a strong conditional dependence on the tag of the previous word. This fact can be exploited in a sequence model such as a hidden Markov model or conditional random field that predicts the entire tag sequence for a sentence, rather than just individual tags, by means of the Viterbi algorithm.\n\n\n== Techniques ==\nProbabilistic graphical models form a large class of structured prediction models. In particular, Bayesian networks and random fields are popular. Other algorithms and models for structured prediction include inductive logic programming, case-based reasoning, structured SVMs, Markov logic networks and constrained conditional models. Main techniques:\n\nConditional random field\nStructured support vector machine\nStructured k-Nearest Neighbours\nRecurrent neural network, in particular Elman network\n\n\n=== Structured perceptron ===\nOne of the easiest ways to understand algorithms for general structured prediction is the structured perceptron of Collins.\nThis algorithm combines the perceptron algorithm for learning linear classifiers with an inference algorithm (classically the Viterbi algorithm when used on sequence data) and can be described abstractly as follows. First define a "joint feature function" \u03a6(x, y) that maps a training sample x and a candidate prediction y to a vector of length n (x and y may have any structure; n is problem-dependent, but must be fixed for each model). Let GEN be a function that generates candidate predictions. Then:\n\nLet \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n   be a weight vector of length nFor a pre-determined number of iterations:For each sample \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   in the training set with true output \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  :Make a prediction \n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n        =\n        \n          \n            a\n            r\n            g\n            \n            m\n            a\n            x\n          \n        \n        \n        {\n        \n          y\n        \n        \u2208\n        \n          G\n          E\n          N\n        \n        (\n        \n          x\n        \n        )\n        }\n        \n        (\n        \n          \n            w\n          \n          \n            T\n          \n        \n        \n        \u03d5\n        (\n        \n          x\n        \n        ,\n        \n          y\n        \n        )\n        )\n      \n    \n    {\\displaystyle {\\hat {y}}={\\operatorname {arg\\,max} }\\,\\{{y}\\in {GEN}({x})\\}\\,({w}^{T}\\,\\phi ({x},{y}))}\n  Update \n  \n    \n      \n        w\n      \n    \n    {\\displaystyle w}\n  , from \n  \n    \n      \n        \n          \n            \n              y\n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {y}}}\n   to \n  \n    \n      \n        t\n      \n    \n    {\\displaystyle t}\n  :  \n  \n    \n      \n        \n          w\n        \n        =\n        \n          w\n        \n        +\n        \n          c\n        \n        (\n        \u2212\n        \u03d5\n        (\n        \n          x\n        \n        ,\n        \n          \n            \n              y\n              ^\n            \n          \n        \n        )\n        +\n        \u03d5\n        (\n        \n          x\n        \n        ,\n        \n          t\n        \n        )\n        )\n      \n    \n    {\\displaystyle {w}={w}+{c}(-\\phi ({x},{\\hat {y}})+\\phi ({x},{t}))}\n  , \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n   is learning rateIn practice, finding the argmax over \n  \n    \n      \n        \n          G\n          E\n          N\n        \n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle {GEN}({x})}\n   will be done using an algorithm such as Viterbi or an algorithm such as max-sum, rather than an exhaustive search through an exponentially large set of candidates.\nThe idea of learning is similar to multiclass perceptron.\n\n\n== References ==\n\nNoah Smith, Linguistic Structure Prediction, 2011.\nMichael Collins, Discriminative Training Methods for Hidden Markov Models, 2002.\n\n\n== External links ==\nImplementation of Collins structured perceptron