An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal \u201cnoise\u201d. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Several variants exist to the basic model, with the aim of forcing the learned representations of the input to assume useful properties. Examples are the regularized autoencoders (Sparse, Denoising and Contractive autoencoders), proven effective in learning representations for subsequent classification tasks, and Variational autoencoders, with their recent applications as generative models. Autoencoders are effectively used for solving many applied problems, from face recognition to acquiring the semantic meaning of words.\n\n\n== Introduction ==\nAn autoencoder  is a neural network that learns to copy its input to its output. It has an internal (hidden) layer that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that maps the code to a reconstruction of the original input.\nPerforming the copying task perfectly would simply duplicate the signal, and this is why autoencoders usually are restricted in ways that force them to reconstruct the input approximately, preserving only the most relevant aspects of the data in the copy.\nThe idea of autoencoders has been popular in the field of neural networks for decades, and the first applications date back to the '80s. Their most traditional application was dimensionality reduction or feature learning, but more recently the autoencoder concept has become more widely used for learning generative models of data. Some of the most powerful AIs in the 2010s involved sparse autoencoders stacked inside of deep neural networks.\n\n\n== Basic Architecture ==\nThe simplest form of an autoencoder is a feedforward, non-recurrent neural network similar to single layer perceptrons that participate in multilayer perceptrons (MLP) \u2013 having an input layer, an output layer and one or more hidden layers connecting them \u2013 where the output layer has the same number of nodes (neurons) as the input layer, and with the purpose of reconstructing its inputs (minimizing the difference between the input and the output) instead of predicting the target value \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given inputs \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  . Therefore, autoencoders are unsupervised learning models (do not require labeled inputs to enable learning).\nAn autoencoder consists of two parts, the encoder and the decoder, which can be defined as transitions \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   and \n  \n    \n      \n        \u03c8\n        ,\n      \n    \n    {\\displaystyle \\psi ,}\n   such that:\n\n  \n    \n      \n        \u03d5\n        :\n        \n          \n            X\n          \n        \n        \u2192\n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle \\phi :{\\mathcal {X}}\\rightarrow {\\mathcal {F}}}\n  \n\n  \n    \n      \n        \u03c8\n        :\n        \n          \n            F\n          \n        \n        \u2192\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle \\psi :{\\mathcal {F}}\\rightarrow {\\mathcal {X}}}\n  \n\n  \n    \n      \n        \u03d5\n        ,\n        \u03c8\n        =\n        \n          \n            \n              a\n              r\n              g\n              \n              m\n              i\n              n\n            \n            \n              \u03d5\n              ,\n              \u03c8\n            \n          \n        \n        \n        \u2016\n        X\n        \u2212\n        (\n        \u03c8\n        \u2218\n        \u03d5\n        )\n        X\n        \n          \u2016\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\phi ,\\psi ={\\underset {\\phi ,\\psi }{\\operatorname {arg\\,min} }}\\,\\|X-(\\psi \\circ \\phi )X\\|^{2}}\n  In the simplest case, given one hidden layer, the encoder stage of an autoencoder takes the input \n  \n    \n      \n        \n          x\n        \n        \u2208\n        \n          \n            R\n          \n          \n            d\n          \n        \n        =\n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} \\in \\mathbb {R} ^{d}={\\mathcal {X}}}\n   and maps it to \n  \n    \n      \n        \n          h\n        \n        \u2208\n        \n          \n            R\n          \n          \n            p\n          \n        \n        =\n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {h} \\in \\mathbb {R} ^{p}={\\mathcal {F}}}\n  :\n\n  \n    \n      \n        \n          h\n        \n        =\n        \u03c3\n        (\n        \n          W\n          x\n        \n        +\n        \n          b\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {h} =\\sigma (\\mathbf {Wx} +\\mathbf {b} )}\n  This image \n  \n    \n      \n        \n          h\n        \n      \n    \n    {\\displaystyle \\mathbf {h} }\n   is usually referred to as code, latent variables, or latent representation. Here, \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   is an element-wise activation function such as a sigmoid function or a rectified linear unit.  \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n   is a weight matrix and \n  \n    \n      \n        \n          b\n        \n      \n    \n    {\\displaystyle \\mathbf {b} }\n   is a bias vector. Weights and biases are usually initialized randomly, and then updated iteratively during training through Backpropagation. After that, the decoder stage of the autoencoder maps \n  \n    \n      \n        \n          h\n        \n      \n    \n    {\\displaystyle \\mathbf {h} }\n   to the reconstruction \n  \n    \n      \n        \n          \n            x\n            \u2032\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x'} }\n   of the same shape as \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  :\n\n  \n    \n      \n        \n          \n            x\n            \u2032\n          \n        \n        =\n        \n          \u03c3\n          \u2032\n        \n        (\n        \n          \n            W\n            \u2032\n          \n          h\n        \n        +\n        \n          \n            b\n            \u2032\n          \n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {x'} =\\sigma '(\\mathbf {W'h} +\\mathbf {b'} )}\n  where \n  \n    \n      \n        \n          \n            \u03c3\n            \u2032\n          \n        \n        ,\n        \n          \n            W\n            \u2032\n          \n        \n        ,\n        \n           and \n        \n        \n          \n            b\n            \u2032\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\sigma '} ,\\mathbf {W'} ,{\\text{ and }}\\mathbf {b'} }\n   for the decoder may be unrelated to the corresponding \n  \n    \n      \n        \n          \u03c3\n        \n        ,\n        \n          W\n        \n        ,\n        \n           and \n        \n        \n          b\n        \n      \n    \n    {\\displaystyle \\mathbf {\\sigma } ,\\mathbf {W} ,{\\text{ and }}\\mathbf {b} }\n   for the encoder.\nAutoencoders are trained to minimise reconstruction errors (such as squared errors), often referred to as the "loss":\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \n            x\n            \u2032\n          \n        \n        )\n        =\n        \u2016\n        \n          x\n        \n        \u2212\n        \n          \n            x\n            \u2032\n          \n        \n        \n          \u2016\n          \n            2\n          \n        \n        =\n        \u2016\n        \n          x\n        \n        \u2212\n        \n          \u03c3\n          \u2032\n        \n        (\n        \n          \n            W\n            \u2032\n          \n        \n        (\n        \u03c3\n        (\n        \n          W\n          x\n        \n        +\n        \n          b\n        \n        )\n        )\n        +\n        \n          \n            b\n            \u2032\n          \n        \n        )\n        \n          \u2016\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x'} )=\\|\\mathbf {x} -\\mathbf {x'} \\|^{2}=\\|\\mathbf {x} -\\sigma '(\\mathbf {W'} (\\sigma (\\mathbf {Wx} +\\mathbf {b} ))+\\mathbf {b'} )\\|^{2}}\n  where \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   is usually averaged over some input training set.\nAs mentioned before, the training of an autoencoder is performed through Backpropagation of the error, just like a regular feedforward neural network.\nShould the feature space \n  \n    \n      \n        \n          \n            F\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {F}}}\n   have lower dimensionality than the input space \n  \n    \n      \n        \n          \n            X\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {X}}}\n  , the feature vector \n  \n    \n      \n        \u03d5\n        (\n        x\n        )\n      \n    \n    {\\displaystyle \\phi (x)}\n   can be regarded as a compressed representation of the input \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  . This is the case of undercomplete autoencoders. If the hidden layers are larger than (overcomplete autoencoders), or equal to, the input layer, or the hidden units are given enough capacity, an autoencoder can potentially learn the identity function and become useless. However, experimental results have shown that autoencoders might still learn useful features in these cases. In the ideal setting, one should be able to tailor the code dimension and the model capacity on the basis of the complexity of the data distribution to be modeled. One way to do so, is to exploit the model variants known as Regularized Autoencoders.\n\n\n== Variations ==\n\n\n=== Regularized Autoencoders ===\nVarious techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations.\n\n\n==== Sparse autoencoder (SAE) ====\n\nRecently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. Sparse autoencoder may include more (rather than fewer) hidden units than inputs, but only a small number of the hidden units are allowed to be active at once. This sparsity constraint forces the model to respond to the unique statistical features of the input data used for training.\nSpecifically, a sparse autoencoder is an autoencoder whose training criterion involves a sparsity penalty \n  \n    \n      \n        \u03a9\n        (\n        \n          h\n        \n        )\n      \n    \n    {\\displaystyle \\Omega ({\\boldsymbol {h}})}\n   on the code layer \n  \n    \n      \n        \n          h\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {h}}}\n  .\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \n            x\n            \u2032\n          \n        \n        )\n        +\n        \u03a9\n        (\n        \n          h\n        \n        )\n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x'} )+\\Omega ({\\boldsymbol {h}})}\n  \nRecalling that \n  \n    \n      \n        \n          h\n        \n        =\n        f\n        (\n        \n          W\n        \n        \n          x\n        \n        +\n        \n          b\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {h}}=f({\\boldsymbol {W}}{\\boldsymbol {x}}+{\\boldsymbol {b}})}\n  , the penalty encourages the model to activate (i.e. output value close to 1) some specific areas of the network on the basis of the input data, while forcing all other neurons to be inactive (i.e. to have an output value close to 0).This sparsity of activation can be achieved by formulating the penalty terms in different ways.\n\nOne way to do it, is by exploiting the Kullback-Leibler (KL) divergence.  Let\n  \n    \n      \n        \n          \n            \n              \n                \u03c1\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n        =\n        \n          \n            1\n            m\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        [\n        \n          h\n          \n            j\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n        ]\n      \n    \n    {\\displaystyle {\\hat {\\rho _{j}}}={\\frac {1}{m}}\\sum _{i=1}^{m}[h_{j}(x_{i})]}\n  \nbe the average activation of the hidden unit \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   (averaged over the  \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   training examples). Note that the notation \n  \n    \n      \n        \n          h\n          \n            j\n          \n        \n        (\n        \n          x\n          \n            i\n          \n        \n        )\n      \n    \n    {\\displaystyle h_{j}(x_{i})}\n   makes explicit what the input affecting the activation was, i.e. it identifies which input value the activation is function of. To encourage most of the neurons to be inactive, we would like \n  \n    \n      \n        \n          \n            \n              \n                \u03c1\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\rho _{j}}}}\n   to be as close to 0 as possible. Therefore, this method enforces the constraint \n  \n    \n      \n        \n          \n            \n              \n                \u03c1\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n        =\n        \u03c1\n      \n    \n    {\\displaystyle {\\hat {\\rho _{j}}}=\\rho }\n    where \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n   is the sparsity parameter, a value close to zero, leading the activation of the hidden units to be mostly zero as well. The penalty term \n  \n    \n      \n        \u03a9\n        (\n        \n          h\n        \n        )\n      \n    \n    {\\displaystyle \\Omega ({\\boldsymbol {h}})}\n   will then take a form that penalizes \n  \n    \n      \n        \n          \n            \n              \n                \u03c1\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\rho _{j}}}}\n   for deviating significantly from \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n  , exploiting the KL divergence:\n\n  \n    \n      \n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            s\n          \n        \n        K\n        L\n        (\n        \u03c1\n        \n          |\n        \n        \n          |\n        \n        \n          \n            \n              \n                \u03c1\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n        )\n        =\n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            s\n          \n        \n        \n          [\n          \n            \u03c1\n            log\n            \u2061\n            \n              \n                \u03c1\n                \n                  \n                    \n                      \u03c1\n                      \n                        j\n                      \n                    \n                    ^\n                  \n                \n              \n            \n            +\n            (\n            1\n            \u2212\n            \u03c1\n            )\n            log\n            \u2061\n            \n              \n                \n                  1\n                  \u2212\n                  \u03c1\n                \n                \n                  1\n                  \u2212\n                  \n                    \n                      \n                        \n                          \u03c1\n                          \n                            j\n                          \n                        \n                        ^\n                      \n                    \n                  \n                \n              \n            \n          \n          ]\n        \n      \n    \n    {\\displaystyle \\sum _{j=1}^{s}KL(\\rho ||{\\hat {\\rho _{j}}})=\\sum _{j=1}^{s}\\left[\\rho \\log {\\frac {\\rho }{\\hat {\\rho _{j}}}}+(1-\\rho )\\log {\\frac {1-\\rho }{1-{\\hat {\\rho _{j}}}}}\\right]}\n    where \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   is summing over the \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   hidden nodes in the hidden layer, and \n  \n    \n      \n        K\n        L\n        (\n        \u03c1\n        \n          |\n        \n        \n          |\n        \n        \n          \n            \n              \n                \u03c1\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n        )\n      \n    \n    {\\displaystyle KL(\\rho ||{\\hat {\\rho _{j}}})}\n   is the KL-divergence between a Bernoulli random variable with mean \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n   and a Bernoulli random variable with mean \n  \n    \n      \n        \n          \n            \n              \n                \u03c1\n                \n                  j\n                \n              \n              ^\n            \n          \n        \n      \n    \n    {\\displaystyle {\\hat {\\rho _{j}}}}\n  .\nAnother way to achieve sparsity in the activation of the hidden unit, is by applying L1 or L2 regularization terms on the activation, scaled by a certain parameter \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  . For instance, in the case of L1 the loss function would become\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \n            x\n            \u2032\n          \n        \n        )\n        +\n        \u03bb\n        \n          \u2211\n          \n            i\n          \n        \n        \n          |\n        \n        \n          h\n          \n            i\n          \n        \n        \n          |\n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x'} )+\\lambda \\sum _{i}|h_{i}|}\n  \n\nA further proposed strategy to force sparsity in the model is that of manually zeroing all but the strongest hidden unit activations (k-sparse autoencoder). The k-sparse autoencoder is based on a linear autoencoder (i.e. with linear activation function) and tied weights. The identification of the strongest activations can be achieved by sorting the activities and keeping only the first k values, or by using ReLU hidden units with thresholds that are adaptively adjusted until the k largest activities are identified. This selection acts like the previously mentioned regularization terms in that it prevents the model from reconstructing the input using too many neurons.\n\n\n==== Denoising autoencoder (DAE) ====\nDifferently from sparse autoencoders or undercomplete autoencoders that constrain representation, Denoising autoencoders (DAE) try to achieve a good representation by changing the reconstruction criterion.Indeed, DAEs take a partially corrupted input and are trained to recover the original undistorted input.  In practice, the objective of denoising autoencoders is that of cleaning the corrupted input, or denoising. Two underlying assumptions are inherent to this approach:\n\nHigher level representations are relatively stable and robust to the corruption of the input;\nTo perform denoising well, the model needs to extract features that capture useful structure in the distribution of the input.In other words, denoising is advocated as a training criterion for learning to extract useful features that will constitute better higher level representations of the input.The training process of a DAE works as follows:\n\nThe initial input \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   is corrupted into \n  \n    \n      \n        \n          \n            \n              x\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\tilde {x}}}}\n   through stochastic mapping \n  \n    \n      \n        \n          \n            \n              x\n              ~\n            \n          \n        \n        \u223c\n        \n          q\n          \n            D\n          \n        \n        (\n        \n          \n            \n              x\n              ~\n            \n          \n        \n        \n          |\n        \n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\tilde {x}}}\\thicksim q_{D}({\\boldsymbol {\\tilde {x}}}|{\\boldsymbol {x}})}\n  .\nThe corrupted input \n  \n    \n      \n        \n          \n            \n              x\n              ~\n            \n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {\\tilde {x}}}}\n   is then mapped to a hidden representation with the same process of the standard autoencoder, \n  \n    \n      \n        \n          h\n        \n        =\n        \n          f\n          \n            \u03b8\n          \n        \n        (\n        \n          \n            \n              x\n              ~\n            \n          \n        \n        )\n        =\n        s\n        (\n        \n          W\n        \n        \n          \n            \n              x\n              ~\n            \n          \n        \n        +\n        \n          b\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {h}}=f_{\\theta }({\\boldsymbol {\\tilde {x}}})=s({\\boldsymbol {W}}{\\boldsymbol {\\tilde {x}}}+{\\boldsymbol {b}})}\n  .\nFrom the hidden representation the model reconstructs \n  \n    \n      \n        \n          z\n        \n        =\n        \n          g\n          \n            \n              \u03b8\n              \u2032\n            \n          \n        \n        (\n        \n          h\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {z}}=g_{\\theta '}({\\boldsymbol {h}})}\n  .The model's parameters \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   and \n  \n    \n      \n        \n          \u03b8\n          \u2032\n        \n      \n    \n    {\\displaystyle \\theta '}\n   are trained to minimize the average reconstruction error over the training data, specifically, minimizing the difference between \n  \n    \n      \n        \n          z\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {z}}}\n   and the original uncorrupted input  \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}}\n  . Note that each time a random example \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}}\n   is presented to the model, a new corrupted version is generated stochastically on the basis of \n  \n    \n      \n        \n          q\n          \n            D\n          \n        \n        (\n        \n          \n            \n              x\n              ~\n            \n          \n        \n        \n          |\n        \n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle q_{D}({\\boldsymbol {\\tilde {x}}}|{\\boldsymbol {x}})}\n  .\nThe above-mentioned training process could be developed with any kind of corruption process. Some examples might be additive isotropic Gaussian noise, Masking noise (a fraction of the input chosen at random for each example is forced to 0) or Salt-and-pepper noise (a fraction of the input chosen at random for each example is set to its minimum or maximum value with uniform probability).Finally, notice that the corruption of the input is performed only during the training phase of the DAE. Once the model has learnt the optimal parameters, in order to extract the representations from the original data no corruption is added.\n\n\n==== Contractive autoencoder (CAE) ====\nContractive autoencoder adds an explicit regularizer in their objective function that forces the model to learn a function that is robust to slight variations of input values. This regularizer corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. Since the penalty is applied to training examples only, this term forces the model to learn useful information about the training distribution. The final objective function has the following form:\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          x\n        \n        ,\n        \n          \n            x\n            \u2032\n          \n        \n        )\n        +\n        \u03bb\n        \n          \u2211\n          \n            i\n          \n        \n        \n          |\n        \n        \n          |\n        \n        \n          \u2207\n          \n            x\n          \n        \n        \n          h\n          \n            i\n          \n        \n        \n          |\n        \n        \n          \n            |\n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x'} )+\\lambda \\sum _{i}||\\nabla _{x}h_{i}||^{2}}\n  The name contractive comes from the fact that the CAE is encouraged to map a neighborhood of input points to a smaller neighborhood of output points.There is a connection between the denoising autoencoder (DAE) and the contractive autoencoder (CAE): in the limit of small Gaussian input noise, DAE make the reconstruction function resist small but finite-sized perturbations of the input, while CAE make the extracted features resist infinitesimal perturbations of the input.\n\n\n=== Variational autoencoder (VAE) ===\nUnlike classical (sparse, denoising, etc.) autoencoders, Variational autoencoders (VAEs) are generative models, like Generative Adversarial Networks. Their association with this group of models derives mainly from the architectural affinity with the basic autoencoder (the final training objective has an encoder and a decoder), but their mathematical formulation differs significantly. VAEs are directed probabilistic graphical models (DPGM) whose posterior is approximated by a neural network, forming an autoencoder-like architecture. Differently from discriminative modeling that aims to learn a predictor given the observation, generative modeling tries to simulate how the data is generated, in order to understand the underlying causal relations. Causal relations have indeed the great potential of being generalizable.Variational autoencoder models make strong assumptions concerning the distribution of latent variables. They use a variational approach for latent representation learning, which results in an additional loss component and a specific estimator for the training algorithm called the Stochastic Gradient Variational Bayes (SGVB) estimator. It assumes that the data is generated by a directed graphical model \n  \n    \n      \n        \n          p\n          \n            \u03b8\n          \n        \n        (\n        \n          x\n        \n        \n          |\n        \n        \n          h\n        \n        )\n      \n    \n    {\\displaystyle p_{\\theta }(\\mathbf {x} |\\mathbf {h} )}\n   and that the encoder is learning an approximation \n  \n    \n      \n        \n          q\n          \n            \u03d5\n          \n        \n        (\n        \n          h\n        \n        \n          |\n        \n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle q_{\\phi }(\\mathbf {h} |\\mathbf {x} )}\n   to the posterior distribution \n  \n    \n      \n        \n          p\n          \n            \u03b8\n          \n        \n        (\n        \n          h\n        \n        \n          |\n        \n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle p_{\\theta }(\\mathbf {h} |\\mathbf {x} )}\n   where \n  \n    \n      \n        \n          \u03d5\n        \n      \n    \n    {\\displaystyle \\mathbf {\\phi } }\n   and \n  \n    \n      \n        \n          \u03b8\n        \n      \n    \n    {\\displaystyle \\mathbf {\\theta } }\n   denote the parameters of the encoder (recognition model) and decoder (generative model) respectively. The probability distribution of the latent vector of a VAE typically matches that of the training data much closer than a standard autoencoder. The objective of VAE has the following form:\n\n  \n    \n      \n        \n          \n            L\n          \n        \n        (\n        \n          \u03d5\n        \n        ,\n        \n          \u03b8\n        \n        ,\n        \n          x\n        \n        )\n        =\n        \n          D\n          \n            \n              K\n              L\n            \n          \n        \n        (\n        \n          q\n          \n            \u03d5\n          \n        \n        (\n        \n          h\n        \n        \n          |\n        \n        \n          x\n        \n        )\n        \u2016\n        \n          p\n          \n            \u03b8\n          \n        \n        (\n        \n          h\n        \n        )\n        )\n        \u2212\n        \n          \n            E\n          \n          \n            \n              q\n              \n                \u03d5\n              \n            \n            (\n            \n              h\n            \n            \n              |\n            \n            \n              x\n            \n            )\n          \n        \n        \n          \n            (\n          \n        \n        log\n        \u2061\n        \n          p\n          \n            \u03b8\n          \n        \n        (\n        \n          x\n        \n        \n          |\n        \n        \n          h\n        \n        )\n        \n          \n            )\n          \n        \n      \n    \n    {\\displaystyle {\\mathcal {L}}(\\mathbf {\\phi } ,\\mathbf {\\theta } ,\\mathbf {x} )=D_{\\mathrm {KL} }(q_{\\phi }(\\mathbf {h} |\\mathbf {x} )\\Vert p_{\\theta }(\\mathbf {h} ))-\\mathbb {E} _{q_{\\phi }(\\mathbf {h} |\\mathbf {x} )}{\\big (}\\log p_{\\theta }(\\mathbf {x} |\\mathbf {h} ){\\big )}}\n  Here, \n  \n    \n      \n        \n          D\n          \n            \n              K\n              L\n            \n          \n        \n      \n    \n    {\\displaystyle D_{\\mathrm {KL} }}\n   stands for the Kullback\u2013Leibler divergence. The prior over the latent variables is usually set to be the centred isotropic multivariate Gaussian \n  \n    \n      \n        \n          p\n          \n            \u03b8\n          \n        \n        (\n        \n          h\n        \n        )\n        =\n        \n          \n            N\n          \n        \n        (\n        \n          0\n          ,\n          I\n        \n        )\n      \n    \n    {\\displaystyle p_{\\theta }(\\mathbf {h} )={\\mathcal {N}}(\\mathbf {0,I} )}\n  ; however, alternative configurations have been considered.Commonly, the shape of the variational and the likelihood distributions are chosen such that they are factorized Gaussians:\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  q\n                  \n                    \u03d5\n                  \n                \n                (\n                \n                  h\n                \n                \n                  |\n                \n                \n                  x\n                \n                )\n              \n              \n                \n                =\n                \n                  \n                    N\n                  \n                \n                (\n                \n                  \u03c1\n                \n                (\n                \n                  x\n                \n                )\n                ,\n                \n                  \n                    \u03c9\n                  \n                  \n                    2\n                  \n                \n                (\n                \n                  x\n                \n                )\n                \n                  I\n                \n                )\n                ,\n              \n            \n            \n              \n                \n                  p\n                  \n                    \u03b8\n                  \n                \n                (\n                \n                  x\n                \n                \n                  |\n                \n                \n                  h\n                \n                )\n              \n              \n                \n                =\n                \n                  \n                    N\n                  \n                \n                (\n                \n                  \u03bc\n                \n                (\n                \n                  h\n                \n                )\n                ,\n                \n                  \n                    \u03c3\n                  \n                  \n                    2\n                  \n                \n                (\n                \n                  h\n                \n                )\n                \n                  I\n                \n                )\n                ,\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}q_{\\phi }(\\mathbf {h} |\\mathbf {x} )&={\\mathcal {N}}({\\boldsymbol {\\rho }}(\\mathbf {x} ),{\\boldsymbol {\\omega }}^{2}(\\mathbf {x} )\\mathbf {I} ),\\\\p_{\\theta }(\\mathbf {x} |\\mathbf {h} )&={\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {h} ),{\\boldsymbol {\\sigma }}^{2}(\\mathbf {h} )\\mathbf {I} ),\\end{aligned}}}\n  where \n  \n    \n      \n        \n          \u03c1\n        \n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\rho }}(\\mathbf {x} )}\n   and  \n  \n    \n      \n        \n          \n            \u03c9\n          \n          \n            2\n          \n        \n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\omega }}^{2}(\\mathbf {x} )}\n   are the encoder outputs, while \n  \n    \n      \n        \n          \u03bc\n        \n        (\n        \n          h\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}(\\mathbf {h} )}\n   and  \n  \n    \n      \n        \n          \n            \u03c3\n          \n          \n            2\n          \n        \n        (\n        \n          h\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\sigma }}^{2}(\\mathbf {h} )}\n   are the decoder outputs.\nThis choice is justified by the simplifications that it produces when evaluating both the KL divergence and the likelihood term in variational objective defined above.\nVAE have been criticized because they generate blurry images. However, researchers employing this model were showing only the mean of the distributions, \n  \n    \n      \n        \n          \u03bc\n        \n        (\n        \n          h\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\mu }}(\\mathbf {h} )}\n  , rather than a sample of the learned Gaussian distribution\n\n  \n    \n      \n        \n          x\n        \n        \u223c\n        \n          \n            N\n          \n        \n        (\n        \n          \u03bc\n        \n        (\n        \n          h\n        \n        )\n        ,\n        \n          \n            \u03c3\n          \n          \n            2\n          \n        \n        (\n        \n          h\n        \n        )\n        \n          I\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {x} \\sim {\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {h} ),{\\boldsymbol {\\sigma }}^{2}(\\mathbf {h} )\\mathbf {I} )}\n  .These samples were shown to be overly noisy due to the choice of a factorized Gaussian distribution. Employing a Gaussian distribution with a full covariance matrix,\n\n  \n    \n      \n        \n          p\n          \n            \u03b8\n          \n        \n        (\n        \n          x\n        \n        \n          |\n        \n        \n          h\n        \n        )\n        =\n        \n          \n            N\n          \n        \n        (\n        \n          \u03bc\n        \n        (\n        \n          h\n        \n        )\n        ,\n        \n          \u03a3\n        \n        (\n        \n          h\n        \n        )\n        )\n        ,\n      \n    \n    {\\displaystyle p_{\\theta }(\\mathbf {x} |\\mathbf {h} )={\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {h} ),{\\boldsymbol {\\Sigma }}(\\mathbf {h} )),}\n  could solve this issue, but is computationally intractable and numerically unstable, as it requires estimating a covariance matrix from a single data sample. However, later research showed that a restricted approach where the inverse matrix \n  \n    \n      \n        \n          \n            \u03a3\n          \n          \n            \u2212\n            1\n          \n        \n        (\n        \n          h\n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {\\Sigma }}^{-1}(\\mathbf {h} )}\n   is sparse, could be tractably employed to generate images with high-frequency details.\nLarge-scale VAE models have been developed in different domains to represent data in a compact probabilistic latent space. For example, VQ-VAE for image generation and Optimus  for language modeling.\n\n\n== Advantages of Depth ==\n\nAutoencoders are often trained with only a single layer encoder and a single layer decoder, but using deep encoders and decoders offers many advantages.\nDepth can exponentially reduce the computational cost of representing some functions.\nDepth can exponentially decrease the amount of training data needed to learn some functions.\nExperimentally, deep autoencoders yield better compression compared to shallow or linear autoencoders.\n\n\n=== Training Deep Architectures ===\nGeoffrey Hinton developed a pretraining technique for training many-layered deep autoencoders. This method involves treating each neighbouring set of two layers as a restricted Boltzmann machine so that the pretraining approximates a good solution, then using a backpropagation technique to fine-tune the results. This model takes the name of deep belief network.\nRecently, researchers have debated whether joint training (i.e. training the whole architecture together with a single global reconstruction objective to optimize) would be better for deep auto-encoders. A study published in 2015 empirically showed that the joint training method not only learns better data models, but also learned more representative features for classification as compared to the layerwise method. However, their experiments highlighted how the success of joint training for deep autoencoder architectures depends heavily on the regularization strategies adopted in the modern variants of the model.\n\n\n== Applications ==\nThe two main applications of autoencoders since the 80s have been dimensionality reduction and information retrieval, but modern variations of the basic model were proven successful when applied to different domains and tasks.\n\n\n=== Dimensionality Reduction ===\nDimensionality Reduction was one of the first applications of deep learning, and one of the early motivations to study autoencoders.  In a nutshell, the objective is to find a proper projection method, that maps data from high feature space to low feature space.One milestone paper on the subject was that of Geoffrey Hinton with his publication in Science Magazine in 2006: in that study, he pretrained a multi-layer autoencoder with a stack of RBMs and then used their weights to initialize a deep autoencoder with gradually smaller hidden layers until a bottleneck of 30 neurons. The resulting 30 dimensions of the code yielded a smaller reconstruction error compared to the first 30 principal components of a PCA, and learned a representation that was qualitatively easier to interpret, clearly separating clusters in the original data.Representing data in a lower-dimensional space can improve performance on different tasks, such as classification. Indeed, many forms of dimensionality reduction place semantically related examples near each other, aiding generalization.\n\n\n==== Relationship with principal component analysis (PCA) ====\n\nIf linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to principal component analysis (PCA). The weights of an autoencoder with a single hidden layer of size \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   (where \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   is less than the size of the input) span the same vector subspace as the one spanned by the first \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components, and are generally not orthogonal, yet the principal components may be recovered from them using the singular value decomposition.However, the potential of Autoencoders resides in their non-linearity, allowing the model to learn more powerful generalizations compared to PCA, and to reconstruct back the input with a significantly lower loss of information.\n\n\n=== Information Retrieval ===\nInformation Retrieval benefits particularly from dimensionality reduction in that search can become extremely efficient in certain kinds of low dimensional spaces. Autoencoders were indeed applied to semantic hashing, proposed by Salakhutdinov and Hinton in 2007. In a nutshell, training the algorithm to produce a low-dimensional binary code, then all database entries could be stored in a hash table mapping binary code vectors to entries. This table would then allow to perform information retrieval by returning all entries with the same binary code as the query, or slightly less similar entries by flipping some bits from the encoding of the query.\n\n\n=== Anomaly Detection ===\nAnother field of application for autoencoders is anomaly detection. By learning to replicate the most salient features in the training data under some of the constraints described previously, the model is encouraged to learn how to precisely reproduce the most frequent characteristics of the observations. When facing anomalies, the model should worsen its reconstruction performance. In most cases, only data with normal instances are used to train the autoencoder; in others, the frequency of anomalies is so small compared to the whole population of observations, that its contribution to the representation learnt by the model could be ignored. After training, the autoencoder will reconstruct normal data very well, while failing to do so with anomaly data which the autoencoder has not encountered. Reconstruction error of a data point, which is the error between the original data point and its low dimensional reconstruction, is used as an anomaly score to detect anomalies.\n\n\n=== Image Processing ===\nThe peculiar characteristics of autoencoders have rendered these model extremely useful in the processing of images for various tasks.\nOne example can be found in lossy image compression task, where autoencoders demonstrated their potential by outperforming other approaches and being proven competitive against JPEG 2000.Another useful application of autoencoders in the field of image preprocessing is image denoising. The need for efficient image restoration methods has grown with the massive production of digital images and movies of all kinds, often taken in poor conditions.Autoencoders are increasingly proving their ability even in more delicate contexts such as medical imaging. In this context, they have also been used for image denoising as well as super-resolution. In the field of image-assisted diagnosis, there exist some experiments using autoencoders for the detection of breast cancer or even modelling the relation between the cognitive decline of Alzheimer's Disease and the latent features of an autoencoder trained with MRILastly, other successful experiments have been carried out exploiting variations of the basic autoencoder for image super-resolution tasks.\n\n\n=== Drug discovery ===\nIn 2019 molecules generated with a special type of variational autoencoders were validated experimentally all the way into mice.\n\n\n=== Population synthesis ===\nIn 2019 a variational autoencoder framework was used to do population synthesis by approximating high-dimensional survey data. By sampling agents from the approximated distribution new synthetic 'fake' populations, with similar statistical properties as those of the original population, were generated.\n\n\n=== Popularity prediction ===\nRecently, stacked autoencoder framework have shown promising results in predicting popularity of social media posts, which is helpful for online advertisement strategies.\n\n\n=== Machine Translation ===\nAutoencoder has been successfully applied to the machine translation of human languages which is usually referred to as neural machine translation (NMT). In NMT, the language texts are treated as sequences to be encoded into the learning procedure, while in the decoder side the target languages will be generated. Recent years also see the application of language specific autoencoders to incorporate the linguistic features into the learning procedure, such as Chinese decomposition features.\n\n\n== See also ==\nRepresentation learning\nSparse dictionary learning\nDeep learning\n\n\n== References ==