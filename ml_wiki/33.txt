CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.\n\n\n== Drawbacks of traditional algorithms ==\nThe popular K-means clustering algorithm minimizes the sum of squared errors criterion:\n\n  \n    \n      \n        E\n        =\n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            k\n          \n        \n        \n          \u2211\n          \n            p\n            \u2208\n            \n              C\n              \n                i\n              \n            \n          \n        \n        (\n        p\n        \u2212\n        \n          m\n          \n            i\n          \n        \n        \n          )\n          \n            2\n          \n        \n        ,\n      \n    \n    {\\displaystyle E=\\sum _{i=1}^{k}\\sum _{p\\in C_{i}}(p-m_{i})^{2},}\n  Given large differences in sizes or geometries of different clusters, the square error method could split the large clusters to minimize the square error, which is not always correct. Also, with hierarchic clustering algorithms these problems exist as none of the distance measures between clusters (\n  \n    \n      \n        \n          d\n          \n            m\n            i\n            n\n          \n        \n        ,\n        \n          d\n          \n            m\n            e\n            a\n            n\n          \n        \n      \n    \n    {\\displaystyle d_{min},d_{mean}}\n  ) tend to work with different cluster shapes.  Also the running time is high when n is large.\nThe problem with the BIRCH algorithm is that once the clusters are generated after step 3, it uses centroids of the clusters and assigns each data point to the cluster with the closest centroid. Using only the centroid to redistribute the data has problems when clusters lack uniform sizes and shapes.\n\n\n== CURE clustering algorithm ==\nTo avoid the problems with non-uniform sized or shaped clusters, CURE employs a hierarchical clustering algorithm that adopts a middle ground between the centroid based and all point extremes. In CURE, a constant number c of well scattered points of a cluster are chosen and they are shrunk towards the centroid of the cluster by a fraction \u03b1. The scattered points after shrinking are used as representatives of the cluster. The clusters with the closest pair of representatives are the clusters that are merged at each step of CURE's hierarchical clustering algorithm. This enables CURE to correctly identify the clusters and makes it less sensitive to outliers.\nRunning time is O(n2 log n), making it rather expensive, and space complexity is O(n).\nThe algorithm cannot be directly applied to large databases because of the high runtime complexity. Enhancements address this requirement.\n\nRandom sampling :  random sampling supports large data sets. Generally the random sample fits in main memory. The random sampling involves a trade off between accuracy and efficiency.\nPartitioning : The basic idea is to partition the sample space into p partitions. Each partition contains n/p elements. The first pass partially clusters each partition until the final number of clusters reduces to n/pq for some constant q \u2265 1. A second clustering pass on n/q partially clusters partitions. For the second pass only the representative points are stored since the merge procedure only requires representative points of previous clusters before computing the representative points for the merged cluster. Partitioning the input reduces the execution times.\nLabeling data on disk : Given only representative points for k clusters, the remaining data points are also assigned to the clusters. For this a fraction of randomly selected representative points for each of the k clusters is chosen and data point is assigned to the cluster containing the representative point closest to it.\n\n\n== Pseudocode ==\nCURE (no. of points,k)\nInput : A set of points S\nOutput : k clusters\n\nFor every cluster u (each input point), in u.mean and u.rep store the mean of the points in the cluster and a set of c representative points of the cluster (initially c = 1 since each cluster has one data point). Also u.closest stores the cluster closest to u.\nAll the input points are inserted into a k-d tree T\nTreat each input point as separate cluster, compute u.closest for each u and then insert each cluster into the heap Q. (clusters are arranged in increasing order of distances between u and u.closest).\nWhile size (Q) > k\nRemove the top element of Q (say u) and merge it with its closest cluster u.closest (say v) and compute the new representative points for the merged cluster w.\nRemove u and v from T and Q.\nFor all the clusters x in Q, update x.closest and relocate x\ninsert w into Q\nrepeat\n\n\n== Availability ==\npyclustering open source library includes a Python and C++ implementation of CURE algorithm.\n\n\n== See also ==\nk-means clustering\nBFR algorithm\n\n\n== References ==\nGuha, Sudipto; Rastogi, Rajeev; Shim, Kyuseok (1998). "CURE: An Efficient Clustering Algorithm for Large Databases" (PDF). Information Systems. 26 (1): 35\u201358. doi:10.1016/S0306-4379(01)00008-4.\nKogan, Jacob; Nicholas, Charles K.; Teboulle, M. (2006). Grouping multidimensional data: recent advances in clustering. Springer. ISBN 978-3-540-28348-5.\nTheodoridis, Sergios; Koutroumbas, Konstantinos (2006). Pattern recognition. Academic Press. pp. 572\u2013574. ISBN 978-0-12-369531-4.