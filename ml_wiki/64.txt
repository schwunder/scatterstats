A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.\nRBMs were initially invented under the name Harmonium by Paul Smolensky in 1986,\nand rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000. RBMs have found applications in dimensionality reduction,classification,collaborative filtering,  feature learning,topic modelling\nand even many body quantum mechanics. They can be trained in either supervised or unsupervised ways, depending on the task.\nAs their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: \na pair of nodes from each of the two groups of units (commonly referred to as the "visible" and "hidden" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, "unrestricted" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by "stacking" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation.\n\n\n== Structure ==\nThe standard type of RBM has binary-valued (Boolean/Bernoulli) hidden and visible units, and consists of a matrix of weights \n  \n    \n      \n        W\n        =\n        (\n        \n          w\n          \n            i\n            ,\n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle W=(w_{i,j})}\n   (size m\xd7n) associated with the connection between hidden unit \n  \n    \n      \n        \n          h\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle h_{j}}\n   and visible unit \n  \n    \n      \n        \n          v\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle v_{i}}\n  , as well as bias weights (offsets) \n  \n    \n      \n        \n          a\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle a_{i}}\n   for the visible units and \n  \n    \n      \n        \n          b\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle b_{j}}\n   for the hidden units. Given these, the energy of a configuration (pair of boolean vectors) (v,h) is defined as\n\n  \n    \n      \n        E\n        (\n        v\n        ,\n        h\n        )\n        =\n        \u2212\n        \n          \u2211\n          \n            i\n          \n        \n        \n          a\n          \n            i\n          \n        \n        \n          v\n          \n            i\n          \n        \n        \u2212\n        \n          \u2211\n          \n            j\n          \n        \n        \n          b\n          \n            j\n          \n        \n        \n          h\n          \n            j\n          \n        \n        \u2212\n        \n          \u2211\n          \n            i\n          \n        \n        \n          \u2211\n          \n            j\n          \n        \n        \n          v\n          \n            i\n          \n        \n        \n          w\n          \n            i\n            ,\n            j\n          \n        \n        \n          h\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle E(v,h)=-\\sum _{i}a_{i}v_{i}-\\sum _{j}b_{j}h_{j}-\\sum _{i}\\sum _{j}v_{i}w_{i,j}h_{j}}\n  or, in matrix notation,\n\n  \n    \n      \n        E\n        (\n        v\n        ,\n        h\n        )\n        =\n        \u2212\n        \n          a\n          \n            \n              T\n            \n          \n        \n        v\n        \u2212\n        \n          b\n          \n            \n              T\n            \n          \n        \n        h\n        \u2212\n        \n          v\n          \n            \n              T\n            \n          \n        \n        W\n        h\n      \n    \n    {\\displaystyle E(v,h)=-a^{\\mathrm {T} }v-b^{\\mathrm {T} }h-v^{\\mathrm {T} }Wh}\n  This energy function is analogous to that of a Hopfield network. As in general Boltzmann machines, probability distributions over hidden and/or visible vectors are defined in terms of the energy function:\n\n  \n    \n      \n        P\n        (\n        v\n        ,\n        h\n        )\n        =\n        \n          \n            1\n            Z\n          \n        \n        \n          e\n          \n            \u2212\n            E\n            (\n            v\n            ,\n            h\n            )\n          \n        \n      \n    \n    {\\displaystyle P(v,h)={\\frac {1}{Z}}e^{-E(v,h)}}\n  where \n  \n    \n      \n        Z\n      \n    \n    {\\displaystyle Z}\n   is a partition function defined as the sum of \n  \n    \n      \n        \n          e\n          \n            \u2212\n            E\n            (\n            v\n            ,\n            h\n            )\n          \n        \n      \n    \n    {\\displaystyle e^{-E(v,h)}}\n   over all possible configurations (in other words, just a normalizing constant to ensure the probability distribution sums to 1). Similarly, the (marginal) probability of a visible (input) vector of booleans is the sum over all possible hidden layer configurations:\n\n  \n    \n      \n        P\n        (\n        v\n        )\n        =\n        \n          \n            1\n            Z\n          \n        \n        \n          \u2211\n          \n            h\n          \n        \n        \n          e\n          \n            \u2212\n            E\n            (\n            v\n            ,\n            h\n            )\n          \n        \n      \n    \n    {\\displaystyle P(v)={\\frac {1}{Z}}\\sum _{h}e^{-E(v,h)}}\n  Since the RBM has the shape of a bipartite graph, with no intra-layer connections, the hidden unit activations are mutually independent given the visible unit activations and conversely, the visible unit activations are mutually independent given the hidden unit activations. That is, for \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   visible units and \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   hidden units, the conditional probability of a configuration of the visible units v, given a configuration of the hidden units h, is\n\n  \n    \n      \n        P\n        (\n        v\n        \n          |\n        \n        h\n        )\n        =\n        \n          \u220f\n          \n            i\n            =\n            1\n          \n          \n            m\n          \n        \n        P\n        (\n        \n          v\n          \n            i\n          \n        \n        \n          |\n        \n        h\n        )\n      \n    \n    {\\displaystyle P(v|h)=\\prod _{i=1}^{m}P(v_{i}|h)}\n  .Conversely, the conditional probability of h given v is\n\n  \n    \n      \n        P\n        (\n        h\n        \n          |\n        \n        v\n        )\n        =\n        \n          \u220f\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        P\n        (\n        \n          h\n          \n            j\n          \n        \n        \n          |\n        \n        v\n        )\n      \n    \n    {\\displaystyle P(h|v)=\\prod _{j=1}^{n}P(h_{j}|v)}\n  .The individual activation probabilities are given by\n\n  \n    \n      \n        P\n        (\n        \n          h\n          \n            j\n          \n        \n        =\n        1\n        \n          |\n        \n        v\n        )\n        =\n        \u03c3\n        \n          (\n          \n            \n              b\n              \n                j\n              \n            \n            +\n            \n              \u2211\n              \n                i\n                =\n                1\n              \n              \n                m\n              \n            \n            \n              w\n              \n                i\n                ,\n                j\n              \n            \n            \n              v\n              \n                i\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle P(h_{j}=1|v)=\\sigma \\left(b_{j}+\\sum _{i=1}^{m}w_{i,j}v_{i}\\right)}\n   and \n  \n    \n      \n        \n        P\n        (\n        \n          v\n          \n            i\n          \n        \n        =\n        1\n        \n          |\n        \n        h\n        )\n        =\n        \u03c3\n        \n          (\n          \n            \n              a\n              \n                i\n              \n            \n            +\n            \n              \u2211\n              \n                j\n                =\n                1\n              \n              \n                n\n              \n            \n            \n              w\n              \n                i\n                ,\n                j\n              \n            \n            \n              h\n              \n                j\n              \n            \n          \n          )\n        \n      \n    \n    {\\displaystyle \\,P(v_{i}=1|h)=\\sigma \\left(a_{i}+\\sum _{j=1}^{n}w_{i,j}h_{j}\\right)}\n  where \n  \n    \n      \n        \u03c3\n      \n    \n    {\\displaystyle \\sigma }\n   denotes the logistic sigmoid.\nThe visible units of Restricted Boltzmann Machine can be multinomial, although the hidden units are Bernoulli. In this case, the logistic function for visible units is replaced by the softmax function\n\n  \n    \n      \n        P\n        (\n        \n          v\n          \n            i\n          \n          \n            k\n          \n        \n        =\n        1\n        \n          |\n        \n        h\n        )\n        =\n        \n          \n            \n              exp\n              \u2061\n              (\n              \n                a\n                \n                  i\n                \n                \n                  k\n                \n              \n              +\n              \n                \u03a3\n                \n                  j\n                \n              \n              \n                W\n                \n                  i\n                  j\n                \n                \n                  k\n                \n              \n              \n                h\n                \n                  j\n                \n              \n              )\n            \n            \n              \n                \u03a3\n                \n                  \n                    k\n                    \u2032\n                  \n                  =\n                  1\n                \n                \n                  K\n                \n              \n              exp\n              \u2061\n              (\n              \n                a\n                \n                  i\n                \n                \n                  \n                    k\n                    \u2032\n                  \n                \n              \n              +\n              \n                \u03a3\n                \n                  j\n                \n              \n              \n                W\n                \n                  i\n                  j\n                \n                \n                  \n                    k\n                    \u2032\n                  \n                \n              \n              \n                h\n                \n                  j\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(v_{i}^{k}=1|h)={\\frac {\\exp(a_{i}^{k}+\\Sigma _{j}W_{ij}^{k}h_{j})}{\\Sigma _{k'=1}^{K}\\exp(a_{i}^{k'}+\\Sigma _{j}W_{ij}^{k'}h_{j})}}}\n  where K is the number of discrete values that the visible values have. They are applied in topic modeling, and recommender systems.\n\n\n=== Relation to other models ===\nRestricted Boltzmann machines are a special case of Boltzmann machines and Markov random fields.\nTheir graphical model corresponds to that of factor analysis.\n\n\n== Training algorithm ==\nRestricted Boltzmann machines are trained to maximize the product of probabilities assigned to some training set \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n   (a matrix, each row of which is treated as a visible vector \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n  ),\n\n  \n    \n      \n        arg\n        \u2061\n        \n          max\n          \n            W\n          \n        \n        \n          \u220f\n          \n            v\n            \u2208\n            V\n          \n        \n        P\n        (\n        v\n        )\n      \n    \n    {\\displaystyle \\arg \\max _{W}\\prod _{v\\in V}P(v)}\n  or equivalently, to maximize the expected log probability of a training sample \n  \n    \n      \n        v\n      \n    \n    {\\displaystyle v}\n   selected randomly from \n  \n    \n      \n        V\n      \n    \n    {\\displaystyle V}\n  :\n\n  \n    \n      \n        arg\n        \u2061\n        \n          max\n          \n            W\n          \n        \n        \n          E\n        \n        \n          [\n          \n            log\n            \u2061\n            P\n            (\n            v\n            )\n          \n          ]\n        \n      \n    \n    {\\displaystyle \\arg \\max _{W}\\mathbb {E} \\left[\\log P(v)\\right]}\n  The algorithm most often used to train RBMs, that is, to optimize the weight vector \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n  , is the contrastive divergence (CD) algorithm due to Hinton, originally developed to train PoE (product of experts) models.\nThe algorithm performs Gibbs sampling and is used inside a gradient descent procedure (similar to the way backpropagation is used inside such a procedure when training feedforward neural nets) to compute weight update.\nThe basic, single-step contrastive divergence (CD-1) procedure for a single sample can be summarized as follows:\n\nTake a training sample v, compute the probabilities of the hidden units and sample a hidden activation vector h from this probability distribution.\nCompute the outer product of v and h and call this the positive gradient.\nFrom h, sample a reconstruction v' of the visible units, then resample the hidden activations h' from this. (Gibbs sampling step)\nCompute the outer product of v' and h' and call this the negative gradient.\nLet the update to the weight matrix \n  \n    \n      \n        W\n      \n    \n    {\\displaystyle W}\n   be the positive gradient minus the negative gradient, times some learning rate: \n  \n    \n      \n        \u0394\n        W\n        =\n        \u03f5\n        (\n        v\n        \n          h\n          \n            \n              T\n            \n          \n        \n        \u2212\n        \n          v\n          \u2032\n        \n        \n          h\n          \n            \u2032\n            \n              \n                T\n              \n            \n          \n        \n        )\n      \n    \n    {\\displaystyle \\Delta W=\\epsilon (vh^{\\mathsf {T}}-v'h'^{\\mathsf {T}})}\n  .\nUpdate the biases a and b analogously: \n  \n    \n      \n        \u0394\n        a\n        =\n        \u03f5\n        (\n        v\n        \u2212\n        \n          v\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle \\Delta a=\\epsilon (v-v')}\n  , \n  \n    \n      \n        \u0394\n        b\n        =\n        \u03f5\n        (\n        h\n        \u2212\n        \n          h\n          \u2032\n        \n        )\n      \n    \n    {\\displaystyle \\Delta b=\\epsilon (h-h')}\n  .A Practical Guide to Training RBMs written by Hinton can be found on his homepage.\n\n\n== See also ==\nAutoencoder\nHelmholtz machine\n\n\n== References ==\n\n\n== External links ==\nIntroduction to Restricted Boltzmann Machines. Edwin Chen's blog, July 18, 2011.\n"A Beginner's Guide to Restricted Boltzmann Machines". Archived from the original on February 11, 2017. Retrieved November 15, 2018.CS1 maint: BOT: original-url status unknown (link). Deeplearning4j Documentation\n"Understanding RBMs". Archived from the original on September 20, 2016. Retrieved December 29, 2014.. Deeplearning4j Documentation\nPython implementation of Bernoulli RBM and tutorial\nSimpleRBM is a very small RBM code (24kB) useful for you to learn about how RBMs learn and work.