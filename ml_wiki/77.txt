In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.In this framework, the learner receives samples and must select a generalization function (called the hypothesis) from a certain class of possible functions. The goal is that, with high probability (the "probably" part), the selected function will have low generalization error (the "approximately correct" part). The learner must be able to learn the concept given any arbitrary approximation ratio, probability of success, or distribution of the samples.\nThe model was later extended to treat noise (misclassified samples).\nAn important innovation of the PAC framework is the introduction of computational complexity theory concepts to machine learning. In particular, the learner is expected to find efficient functions (time and space requirements bounded to a polynomial of the example size), and the learner itself must implement an efficient procedure (requiring an example count bounded to a polynomial of the concept size, modified by the approximation and likelihood bounds).\n\n\n== Definitions and terminology ==\nIn order to give the definition for something that is PAC-learnable, we first have to introduce some terminology.For the following definitions, two examples will be used.  The first is the problem of character recognition given an array of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   bits encoding a binary-valued image.  The other example is the problem of finding an interval that will correctly classify points within the interval as positive and the points outside of the range as negative.\nLet \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   be a set called the instance space or the encoding of all the samples.  In the character recognition problem, the instance space is \n  \n    \n      \n        X\n        =\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X=\\{0,1\\}^{n}}\n  .  In the interval problem the instance space, \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , is the set of all bounded intervals in \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\mathbb {R} }\n  , where \n  \n    \n      \n        \n          R\n        \n      \n    \n    {\\displaystyle \\mathbb {R} }\n   denotes the set of all real numbers.\nA concept is a subset \n  \n    \n      \n        c\n        \u2282\n        X\n      \n    \n    {\\displaystyle c\\subset X}\n  .  One concept is the set of all patterns of bits in \n  \n    \n      \n        X\n        =\n        {\n        0\n        ,\n        1\n        \n          }\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X=\\{0,1\\}^{n}}\n   that encode a picture of the letter "P".  An example concept from the second example is the set of open intervals, \n  \n    \n      \n        {\n        (\n        a\n        ,\n        b\n        )\n        \u2223\n        0\n        \u2264\n        a\n        \u2264\n        \u03c0\n        \n          /\n        \n        2\n        ,\n        \u03c0\n        \u2264\n        b\n        \u2264\n        \n          \n            13\n          \n        \n        }\n      \n    \n    {\\displaystyle \\{(a,b)\\mid 0\\leq a\\leq \\pi /2,\\pi \\leq b\\leq {\\sqrt {13}}\\}}\n  , each of which contain only the positive points.  A concept class  \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   is a set of concepts over \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  .  This could be the set of all subsets of the array of bits that are skeletonized 4-connected (width of the font is 1).\nLet \n  \n    \n      \n        E\n        X\n        (\n        c\n        ,\n        D\n        )\n      \n    \n    {\\displaystyle EX(c,D)}\n   be a procedure that draws an example, \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , using a probability distribution \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   and gives the correct label \n  \n    \n      \n        c\n        (\n        x\n        )\n      \n    \n    {\\displaystyle c(x)}\n  , that is 1 if \n  \n    \n      \n        x\n        \u2208\n        c\n      \n    \n    {\\displaystyle x\\in c}\n   and 0 otherwise.\nNow, given \n  \n    \n      \n        0\n        <\n        \u03f5\n        ,\n        \u03b4\n        <\n        1\n      \n    \n    {\\displaystyle 0<\\epsilon ,\\delta <1}\n  , assume there is an algorithm \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   and a polynomial \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   in \n  \n    \n      \n        1\n        \n          /\n        \n        \u03f5\n        ,\n        1\n        \n          /\n        \n        \u03b4\n      \n    \n    {\\displaystyle 1/\\epsilon ,1/\\delta }\n   (and other relevant parameters of the class \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  ) such that, given a sample of size \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   drawn according to \n  \n    \n      \n        E\n        X\n        (\n        c\n        ,\n        D\n        )\n      \n    \n    {\\displaystyle EX(c,D)}\n  , then, with probability of at least \n  \n    \n      \n        1\n        \u2212\n        \u03b4\n      \n    \n    {\\displaystyle 1-\\delta }\n  , \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   outputs a hypothesis \n  \n    \n      \n        h\n        \u2208\n        C\n      \n    \n    {\\displaystyle h\\in C}\n   that has an average error less than or equal to \n  \n    \n      \n        \u03f5\n      \n    \n    {\\displaystyle \\epsilon }\n   on \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   with the same distribution \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n  .  Further if the above statement for algorithm \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n  \nis true for every concept \n  \n    \n      \n        c\n        \u2208\n        C\n      \n    \n    {\\displaystyle c\\in C}\n   and for every distribution \n  \n    \n      \n        D\n      \n    \n    {\\displaystyle D}\n   over \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n  , and for all \n  \n    \n      \n        0\n        <\n        \u03f5\n        ,\n        \u03b4\n        <\n        1\n      \n    \n    {\\displaystyle 0<\\epsilon ,\\delta <1}\n    then \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n   is (efficiently) PAC learnable (or distribution-free PAC learnable).  We can also say that \n  \n    \n      \n        A\n      \n    \n    {\\displaystyle A}\n   is a PAC learning algorithm for \n  \n    \n      \n        C\n      \n    \n    {\\displaystyle C}\n  .\n\n\n== Equivalence ==\nUnder some regularity conditions these conditions are equivalent: \nThe concept class C is PAC learnable.\nThe VC dimension of C is finite.\nC is a uniform Glivenko-Cantelli class.\nC is compressible in the sense of Littlestone and Warmuth\n\n\n== See also ==\nMachine learning\nData mining\nError tolerance (PAC learning)\nSample complexity\n\n\n== References ==\n\nhttps://users.soe.ucsc.edu/~manfred/pubs/lrnk-olivier.pdf\nA bot will complete this citation soon. Click here to jump the queue arXiv:1503.06960.\n\n\n== Further reading ==\nM. Kearns, U. Vazirani. An Introduction to Computational Learning Theory. MIT Press, 1994. A textbook.\nM. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. MIT Press, 2018. Chapter 2 contains a detailed treatment of PAC-learnability. Readable through open access from the publisher.\nD. Haussler. Overview of the Probably Approximately Correct (PAC) Learning Framework. An introduction to the topic.\nL. Valiant. Probably Approximately Correct. Basic Books, 2013. In which Valiant argues that PAC learning describes how organisms evolve and learn.