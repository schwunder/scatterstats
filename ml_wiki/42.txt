In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the "cocktail party problem" of listening in on one person's speech in a noisy room.\n\n\n== Introduction ==\n\nIndependent component analysis attempts to decompose a multivariate signal into independent non-Gaussian signals. As an example, sound is usually a signal that is composed of the numerical addition, at each time t, of signals from several sources. The question then is whether it is possible to separate these contributing sources from the observed total signal. When the statistical independence assumption is correct, blind ICA separation of a mixed signal gives very good results. It is also used for signals that are not supposed to be generated by mixing for analysis purposes.\nA simple application of ICA is the "cocktail party problem", where the underlying speech signals are separated from a sample data consisting of people talking simultaneously in a room. Usually the problem is simplified by assuming no time delays or echoes. Note that a filtered and delayed signal is a copy of a dependent component, and thus the statistical independence assumption is not violated.\nMixing weights for constructing the \n  \n    \n      \n        M\n      \n    \n    {\\textstyle M}\n   observed signals from the \n  \n    \n      \n        N\n      \n    \n    {\\textstyle N}\n   components can be placed in an \n  \n    \n      \n        M\n        \xd7\n        N\n      \n    \n    {\\textstyle M\\times N}\n   matrix. An important thing to consider is that if \n  \n    \n      \n        N\n      \n    \n    {\\textstyle N}\n   sources are present, at least \n  \n    \n      \n        N\n      \n    \n    {\\textstyle N}\n   observations (e.g. microphones if the observed signal is audio) are needed to recover the original signals. When there are an equal number of observations and source signals, the mixing matrix is square (\n  \n    \n      \n        M\n        =\n        N\n      \n    \n    {\\textstyle M=N}\n  ). Other cases of underdetermined (\n  \n    \n      \n        M\n        <\n        N\n      \n    \n    {\\textstyle M<N}\n  ) and overdetermined (\n  \n    \n      \n        M\n        >\n        N\n      \n    \n    {\\textstyle M>N}\n  ) have been investigated.\nThat the ICA separation of mixed signals gives very good results is based on two assumptions and three effects of mixing source signals. Two assumptions:\n\nThe source signals are independent of each other.\nThe values in each source signal have non-Gaussian distributions.Three effects of mixing source signals:\n\nIndependence: As per assumption 1, the source signals are independent; however, their signal mixtures are not. This is because the signal mixtures share the same source signals.\nNormality: According to the Central Limit Theorem, the distribution of a sum of independent random variables with finite variance tends towards a Gaussian distribution.Loosely speaking, a sum of two independent random variables usually has a distribution that is closer to Gaussian than any of the two original variables. Here we consider the value of each signal as the random variable.\nComplexity: The temporal complexity of any signal mixture is greater than that of its simplest constituent source signal.Those principles contribute to the basic establishment of ICA. If the signals we happen to extract from a set of mixtures are independent like sources signals, and have non-Gaussian histograms or have low complexity like source signals, then they must be source signals.\n\n\n== Defining component independence ==\nICA finds the independent components (also called factors, latent variables or sources) by maximizing the statistical independence of the estimated components. We may choose one of many ways to define a proxy for independence, and this choice governs the form of the ICA algorithm. The two broadest definitions of independence for ICA are\n\nMinimization of mutual information\nMaximization of non-GaussianityThe Minimization-of-Mutual information (MMI) family of ICA algorithms uses measures like Kullback-Leibler Divergence and maximum entropy.  The non-Gaussianity family of ICA algorithms, motivated by the central limit theorem, uses kurtosis and negentropy.\nTypical algorithms for ICA use centering (subtract the mean to create a zero mean signal), whitening (usually with the eigenvalue decomposition), and dimensionality reduction as preprocessing steps in order to simplify and reduce the complexity of the problem for the actual iterative algorithm. Whitening and dimension reduction can be achieved with principal component analysis or singular value decomposition. Whitening ensures that all dimensions are treated equally a priori before the algorithm is run. Well-known algorithms for ICA include infomax, FastICA, JADE, and kernel-independent component analysis, among others. In general, ICA cannot identify the actual number of source signals, a uniquely correct ordering of the source signals, nor the proper scaling (including sign) of the source signals.\nICA is important to blind signal separation and has many practical applications. It is closely related to (or even a special case of) the search for a factorial code of the data, i.e., a new vector-valued representation of each data vector such that it gets uniquely encoded by the resulting code vector (loss-free coding), but the code components are statistically independent.\n\n\n== Mathematical definitions ==\nLinear independent component analysis can be divided into noiseless and noisy cases, where noiseless ICA is a special case of noisy ICA. Nonlinear ICA should be considered as a separate case.\n\n\n=== General definition ===\nThe data are represented by the observed random vector \n  \n    \n      \n        \n          x\n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            m\n          \n        \n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}=(x_{1},\\ldots ,x_{m})^{T}}\n   and the hidden components as the random vector \n  \n    \n      \n        \n          s\n        \n        =\n        (\n        \n          s\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          s\n          \n            n\n          \n        \n        \n          )\n          \n            T\n          \n        \n        .\n      \n    \n    {\\displaystyle {\\boldsymbol {s}}=(s_{1},\\ldots ,s_{n})^{T}.}\n   The task is to transform the observed data \n  \n    \n      \n        \n          x\n        \n        ,\n      \n    \n    {\\displaystyle {\\boldsymbol {x}},}\n   using a linear static transformation \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {W}}}\n   as \n  \n    \n      \n        \n          s\n        \n        =\n        \n          W\n        \n        \n          x\n        \n        ,\n      \n    \n    {\\displaystyle {\\boldsymbol {s}}={\\boldsymbol {W}}{\\boldsymbol {x}},}\n   into a vector of maximally independent components \n  \n    \n      \n        \n          s\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {s}}}\n   measured by some function \n  \n    \n      \n        F\n        (\n        \n          s\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          s\n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle F(s_{1},\\ldots ,s_{n})}\n   of independence.\n\n\n=== Generative model ===\n\n\n==== Linear noiseless ICA ====\nThe components \n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle x_{i}}\n   of the observed random vector \n  \n    \n      \n        \n          x\n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            m\n          \n        \n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}=(x_{1},\\ldots ,x_{m})^{T}}\n   are generated as a sum of the independent components \n  \n    \n      \n        \n          s\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle s_{k}}\n  , \n  \n    \n      \n        k\n        =\n        1\n        ,\n        \u2026\n        ,\n        n\n      \n    \n    {\\displaystyle k=1,\\ldots ,n}\n  :\n\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        =\n        \n          a\n          \n            i\n            ,\n            1\n          \n        \n        \n          s\n          \n            1\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            i\n            ,\n            k\n          \n        \n        \n          s\n          \n            k\n          \n        \n        +\n        \u22ef\n        +\n        \n          a\n          \n            i\n            ,\n            n\n          \n        \n        \n          s\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle x_{i}=a_{i,1}s_{1}+\\cdots +a_{i,k}s_{k}+\\cdots +a_{i,n}s_{n}}\n  \nweighted by the mixing weights \n  \n    \n      \n        \n          a\n          \n            i\n            ,\n            k\n          \n        \n      \n    \n    {\\displaystyle a_{i,k}}\n  .\nThe same generative model can be written in vector form as \n  \n    \n      \n        \n          x\n        \n        =\n        \n          \u2211\n          \n            k\n            =\n            1\n          \n          \n            n\n          \n        \n        \n          s\n          \n            k\n          \n        \n        \n          \n            a\n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}=\\sum _{k=1}^{n}s_{k}{\\boldsymbol {a}}_{k}}\n  , where the observed random vector \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}}\n   is represented by the basis vectors \n  \n    \n      \n        \n          \n            a\n          \n          \n            k\n          \n        \n        =\n        (\n        \n          \n            a\n          \n          \n            1\n            ,\n            k\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \n            a\n          \n          \n            m\n            ,\n            k\n          \n        \n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {a}}_{k}=({\\boldsymbol {a}}_{1,k},\\ldots ,{\\boldsymbol {a}}_{m,k})^{T}}\n  . The basis vectors \n  \n    \n      \n        \n          \n            a\n          \n          \n            k\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {a}}_{k}}\n   form the columns of the mixing matrix \n  \n    \n      \n        \n          A\n        \n        =\n        (\n        \n          \n            a\n          \n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \n            a\n          \n          \n            n\n          \n        \n        )\n      \n    \n    {\\displaystyle {\\boldsymbol {A}}=({\\boldsymbol {a}}_{1},\\ldots ,{\\boldsymbol {a}}_{n})}\n   and the generative formula can be written as \n  \n    \n      \n        \n          x\n        \n        =\n        \n          A\n        \n        \n          s\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}={\\boldsymbol {A}}{\\boldsymbol {s}}}\n  , where \n  \n    \n      \n        \n          s\n        \n        =\n        (\n        \n          s\n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          s\n          \n            n\n          \n        \n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {s}}=(s_{1},\\ldots ,s_{n})^{T}}\n  .\nGiven the model and realizations (samples) \n  \n    \n      \n        \n          \n            x\n          \n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \n            x\n          \n          \n            N\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}_{1},\\ldots ,{\\boldsymbol {x}}_{N}}\n   of the random vector \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}}\n  , the task is to estimate both the mixing matrix \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {A}}}\n   and the sources \n  \n    \n      \n        \n          s\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {s}}}\n  . This is done by adaptively calculating the \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {w}}}\n   vectors and setting up a cost function which either maximizes the non-gaussianity of the calculated \n  \n    \n      \n        \n          s\n          \n            k\n          \n        \n        =\n        \n          \n            w\n          \n          \n            T\n          \n        \n        \n          x\n        \n      \n    \n    {\\displaystyle s_{k}={\\boldsymbol {w}}^{T}{\\boldsymbol {x}}}\n   or minimizes the mutual information. In some cases, a priori knowledge of the probability distributions of the sources can be used in the cost function.\nThe original sources \n  \n    \n      \n        \n          s\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {s}}}\n   can be recovered by multiplying the observed signals \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {x}}}\n   with the inverse of the mixing matrix \n  \n    \n      \n        \n          W\n        \n        =\n        \n          \n            A\n          \n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle {\\boldsymbol {W}}={\\boldsymbol {A}}^{-1}}\n  , also known as the unmixing matrix. Here it is assumed that the mixing matrix is square (\n  \n    \n      \n        n\n        =\n        m\n      \n    \n    {\\displaystyle n=m}\n  ). If the number of basis vectors is greater than the dimensionality of the observed vectors, \n  \n    \n      \n        n\n        >\n        m\n      \n    \n    {\\displaystyle n>m}\n  , the task is overcomplete but is still solvable with the pseudo inverse.\n\n\n==== Linear noisy ICA ====\nWith the added assumption of zero-mean and uncorrelated Gaussian noise \n  \n    \n      \n        n\n        \u223c\n        N\n        (\n        0\n        ,\n        diag\n        \u2061\n        (\n        \u03a3\n        )\n        )\n      \n    \n    {\\displaystyle n\\sim N(0,\\operatorname {diag} (\\Sigma ))}\n  , the ICA model takes the form \n  \n    \n      \n        \n          x\n        \n        =\n        \n          A\n        \n        \n          s\n        \n        +\n        n\n      \n    \n    {\\displaystyle {\\boldsymbol {x}}={\\boldsymbol {A}}{\\boldsymbol {s}}+n}\n  .\n\n\n==== Nonlinear ICA ====\nThe mixing of the sources does not need to be linear. Using a nonlinear mixing function \n  \n    \n      \n        f\n        (\n        \u22c5\n        \n          |\n        \n        \u03b8\n        )\n      \n    \n    {\\displaystyle f(\\cdot |\\theta )}\n   with parameters \n  \n    \n      \n        \u03b8\n      \n    \n    {\\displaystyle \\theta }\n   the nonlinear ICA model is \n  \n    \n      \n        x\n        =\n        f\n        (\n        s\n        \n          |\n        \n        \u03b8\n        )\n        +\n        n\n      \n    \n    {\\displaystyle x=f(s|\\theta )+n}\n  .\n\n\n=== Identifiability ===\nThe independent components are identifiable up to a permutation and scaling of the sources. This identifiability requires that:\n\nAt most one of the sources \n  \n    \n      \n        \n          s\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle s_{k}}\n   is Gaussian,\nThe number of observed mixtures, \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n  , must be at least as large as the number of estimated components \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n  : \n  \n    \n      \n        m\n        \u2265\n        n\n      \n    \n    {\\displaystyle m\\geq n}\n  . It is equivalent to say that the mixing matrix \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle {\\boldsymbol {A}}}\n   must be of full rank for its inverse to exist.\n\n\n== Binary ICA ==\nA special variant of ICA is binary ICA in which both signal sources and monitors are in binary form and observations from monitors are disjunctive mixtures of binary independent sources. The problem was shown to have applications in many domains including medical diagnosis, multi-cluster assignment, network tomography and internet resource management.\nLet \n  \n    \n      \n        \n          \n            x\n            \n              1\n            \n          \n          ,\n          \n            x\n            \n              2\n            \n          \n          ,\n          \u2026\n          ,\n          \n            x\n            \n              m\n            \n          \n        \n      \n    \n    {\\displaystyle {x_{1},x_{2},\\ldots ,x_{m}}}\n   be the set of binary variables from \n  \n    \n      \n        m\n      \n    \n    {\\displaystyle m}\n   monitors and \n  \n    \n      \n        \n          \n            y\n            \n              1\n            \n          \n          ,\n          \n            y\n            \n              2\n            \n          \n          ,\n          \u2026\n          ,\n          \n            y\n            \n              n\n            \n          \n        \n      \n    \n    {\\displaystyle {y_{1},y_{2},\\ldots ,y_{n}}}\n   be the set of binary variables from \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   sources. Source-monitor connections are represented by the (unknown) mixing matrix \n  \n    \n      \n        \n          G\n        \n      \n    \n    {\\textstyle {\\boldsymbol {G}}}\n  , where \n  \n    \n      \n        \n          g\n          \n            i\n            j\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle g_{ij}=1}\n   indicates that signal from the i-th source can be observed by the j-th monitor. The system works as follows: at any time, if a source \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n   is active (\n  \n    \n      \n        \n          y\n          \n            i\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle y_{i}=1}\n  ) and it is connected to the monitor \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   (\n  \n    \n      \n        \n          g\n          \n            i\n            j\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle g_{ij}=1}\n  ) then the monitor \n  \n    \n      \n        j\n      \n    \n    {\\displaystyle j}\n   will observe some activity (\n  \n    \n      \n        \n          x\n          \n            j\n          \n        \n        =\n        1\n      \n    \n    {\\displaystyle x_{j}=1}\n  ). Formally we have:\n\n  \n    \n      \n        \n          x\n          \n            i\n          \n        \n        =\n        \n          \u22c1\n          \n            j\n            =\n            1\n          \n          \n            n\n          \n        \n        (\n        \n          g\n          \n            i\n            j\n          \n        \n        \u2227\n        \n          y\n          \n            j\n          \n        \n        )\n        ,\n        i\n        =\n        1\n        ,\n        2\n        ,\n        \u2026\n        ,\n        m\n        ,\n      \n    \n    {\\displaystyle x_{i}=\\bigvee _{j=1}^{n}(g_{ij}\\wedge y_{j}),i=1,2,\\ldots ,m,}\n  where \n  \n    \n      \n        \u2227\n      \n    \n    {\\displaystyle \\wedge }\n   is Boolean AND and \n  \n    \n      \n        \u2228\n      \n    \n    {\\displaystyle \\vee }\n   is Boolean OR. Note that noise is not explicitly modelled, rather, can be treated as independent sources.\nThe above problem can be heuristically solved  by assuming variables are continuous and running FastICA on binary observation data to get the mixing matrix \n  \n    \n      \n        \n          G\n        \n      \n    \n    {\\textstyle {\\boldsymbol {G}}}\n   (real values), then apply round number techniques on \n  \n    \n      \n        \n          G\n        \n      \n    \n    {\\textstyle {\\boldsymbol {G}}}\n   to obtain the binary values. This approach has been shown to produce a highly inaccurate result.Another method is to use dynamic programming: recursively breaking the observation matrix \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\textstyle {\\boldsymbol {X}}}\n   into its sub-matrices and run the inference algorithm on these sub-matrices. The key observation which leads to this algorithm is the sub-matrix \n  \n    \n      \n        \n          \n            X\n          \n          \n            0\n          \n        \n      \n    \n    {\\textstyle {\\boldsymbol {X}}^{0}}\n   of \n  \n    \n      \n        \n          X\n        \n      \n    \n    {\\textstyle {\\boldsymbol {X}}}\n   where \n  \n    \n      \n        \n          x\n          \n            i\n            j\n          \n        \n        =\n        0\n        ,\n        \u2200\n        j\n      \n    \n    {\\textstyle x_{ij}=0,\\forall j}\n   corresponds to the unbiased observation matrix of hidden components that do not have connection to the \n  \n    \n      \n        i\n      \n    \n    {\\displaystyle i}\n  -th monitor. Experimental results from  show that this approach is accurate under moderate noise levels.\nThe Generalized Binary ICA framework  introduces a broader problem formulation which does not necessitate any knowledge on the generative model. In other words, this method attempts to decompose a source into its independent components (as much as possible, and without losing any information) with no prior assumption on the way it was generated. Although this problem appears quite complex, it can be accurately solved with a branch and bound search tree algorithm or tightly upper bounded with a single multiplication of a matrix with a vector.\n\n\n== Methods for blind source separation ==\n\n\n=== Projection pursuit ===\nSignal mixtures tend to have Gaussian probability density functions, and source signals tend to have non-Gaussian probability density functions.  Each source signal can be extracted from a set of signal mixtures by taking the inner product of a weight vector and those signal mixtures where this inner product provides an orthogonal projection of the signal mixtures.  The remaining challenge is finding such a weight vector. One type of method for doing so is projection pursuit.Projection pursuit seeks one projection at a time such that the extracted signal is as non-Gaussian as possible. This contrasts with ICA, which typically extracts M signals simultaneously from M signal mixtures, which requires estimating a M \xd7 M unmixing matrix. One practical advantage of projection pursuit over ICA is that fewer than M signals can be extracted if required, where each source signal is extracted from M signal mixtures using an M-element weight vector.\nWe can use kurtosis to recover the multiple source signal by finding the correct weight vectors with the use of projection pursuit.\nThe kurtosis of the probability density function of a signal, for a finite sample, is computed as\n\n  \n    \n      \n        K\n        =\n        \n          \n            \n              E\n              \u2061\n              [\n              (\n              \n                y\n              \n              \u2212\n              \n                \n                  y\n                  \xaf\n                \n              \n              \n                )\n                \n                  4\n                \n              \n              ]\n            \n            \n              (\n              E\n              \u2061\n              [\n              (\n              \n                y\n              \n              \u2212\n              \n                \n                  y\n                  \xaf\n                \n              \n              \n                )\n                \n                  2\n                \n              \n              ]\n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n        \u2212\n        3\n      \n    \n    {\\displaystyle K={\\frac {\\operatorname {E} [(\\mathbf {y} -\\mathbf {\\overline {y}} )^{4}]}{(\\operatorname {E} [(\\mathbf {y} -\\mathbf {\\overline {y}} )^{2}])^{2}}}-3}\n  where \n  \n    \n      \n        \n          \n            y\n            \xaf\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {\\overline {y}} }\n   is the sample mean of \n  \n    \n      \n        \n          y\n        \n      \n    \n    {\\displaystyle \\mathbf {y} }\n  , the extracted signals. The constant 3 ensures that Gaussian signals have zero kurtosis, Super-Gaussian signals have positive kurtosis, and Sub-Gaussian signals have negative kurtosis. The denominator is the variance of \n  \n    \n      \n        \n          y\n        \n      \n    \n    {\\displaystyle \\mathbf {y} }\n  , and ensures that the measured kurtosis takes account of signal variance. The goal of projection pursuit is to maximize the kurtosis, and make the extracted signal as non-normal as possible.\nUsing kurtosis as a measure of non-normality, we can now examine how the kurtosis of a signal \n  \n    \n      \n        \n          y\n        \n        =\n        \n          \n            w\n          \n          \n            T\n          \n        \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {y} =\\mathbf {w} ^{T}\\mathbf {x} }\n   extracted from a set of M mixtures \n  \n    \n      \n        \n          x\n        \n        =\n        (\n        \n          x\n          \n            1\n          \n        \n        ,\n        \n          x\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          x\n          \n            M\n          \n        \n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} =(x_{1},x_{2},\\ldots ,x_{M})^{T}}\n   varies as the weight vector \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   is rotated around the origin. Given our assumption that each source signal \n  \n    \n      \n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathbf {s} }\n   is super-gaussian we would expect:\n\nthe kurtosis of the extracted signal \n  \n    \n      \n        \n          y\n        \n      \n    \n    {\\displaystyle \\mathbf {y} }\n   to be maximal precisely when \n  \n    \n      \n        \n          y\n        \n        =\n        \n          s\n        \n      \n    \n    {\\displaystyle \\mathbf {y} =\\mathbf {s} }\n  .\nthe kurtosis of the extracted signal \n  \n    \n      \n        \n          y\n        \n      \n    \n    {\\displaystyle \\mathbf {y} }\n   to be maximal when \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   is orthogonal to the projected axes \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle S_{1}}\n   or \n  \n    \n      \n        \n          S\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle S_{2}}\n  , because we know the optimal weight vector should be orthogonal to a transformed axis \n  \n    \n      \n        \n          S\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle S_{1}}\n   or \n  \n    \n      \n        \n          S\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle S_{2}}\n  .For multiple source mixture signals, we can use kurtosis and Gram-Schmidt Orthogonalization (GSO) to recover the signals. Given M signal mixtures in an M-dimensional space, GSO project these data points onto an (M-1)-dimensional space by using the weight vector. We can guarantee the independence of the extracted signals with the use of GSO.\nIn order to find the correct value of \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n  , we can use gradient descent method.  We first of all whiten the data, and transform \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   into a new mixture \n  \n    \n      \n        \n          z\n        \n      \n    \n    {\\displaystyle \\mathbf {z} }\n  , which has unit variance, and \n  \n    \n      \n        \n          z\n        \n        =\n        (\n        \n          z\n          \n            1\n          \n        \n        ,\n        \n          z\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          z\n          \n            M\n          \n        \n        \n          )\n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {z} =(z_{1},z_{2},\\ldots ,z_{M})^{T}}\n  . This process can be achieved by applying Singular value decomposition to \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  ,\n\n  \n    \n      \n        \n          x\n        \n        =\n        \n          U\n        \n        \n          D\n        \n        \n          \n            V\n          \n          \n            T\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} =\\mathbf {U} \\mathbf {D} \\mathbf {V} ^{T}}\n  Rescaling each vector \n  \n    \n      \n        \n          U\n          \n            i\n          \n        \n        =\n        \n          U\n          \n            i\n          \n        \n        \n          /\n        \n        E\n        \u2061\n        (\n        \n          U\n          \n            i\n          \n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle U_{i}=U_{i}/\\operatorname {E} (U_{i}^{2})}\n  , and let \n  \n    \n      \n        \n          z\n        \n        =\n        \n          U\n        \n      \n    \n    {\\displaystyle \\mathbf {z} =\\mathbf {U} }\n  . The signal extracted by a weighted vector \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   is \n  \n    \n      \n        \n          y\n        \n        =\n        \n          \n            w\n          \n          \n            T\n          \n        \n        \n          z\n        \n      \n    \n    {\\displaystyle \\mathbf {y} =\\mathbf {w} ^{T}\\mathbf {z} }\n  . If the weight vector w has unit length, that is \n  \n    \n      \n        E\n        \u2061\n        [\n        (\n        \n          \n            w\n          \n          \n            T\n          \n        \n        \n          z\n        \n        \n          )\n          \n            2\n          \n        \n        ]\n        =\n        1\n      \n    \n    {\\displaystyle \\operatorname {E} [(\\mathbf {w} ^{T}\\mathbf {z} )^{2}]=1}\n  , then the kurtosis can be written as:\n\n  \n    \n      \n        K\n        =\n        \n          \n            \n              E\n              \u2061\n              [\n              \n                \n                  y\n                \n                \n                  4\n                \n              \n              ]\n            \n            \n              (\n              E\n              \u2061\n              [\n              \n                \n                  y\n                \n                \n                  2\n                \n              \n              ]\n              \n                )\n                \n                  2\n                \n              \n            \n          \n        \n        \u2212\n        3\n        =\n        E\n        \u2061\n        [\n        (\n        \n          \n            w\n          \n          \n            T\n          \n        \n        \n          z\n        \n        \n          )\n          \n            4\n          \n        \n        ]\n        \u2212\n        3.\n      \n    \n    {\\displaystyle K={\\frac {\\operatorname {E} [\\mathbf {y} ^{4}]}{(\\operatorname {E} [\\mathbf {y} ^{2}])^{2}}}-3=\\operatorname {E} [(\\mathbf {w} ^{T}\\mathbf {z} )^{4}]-3.}\n  The updating process for \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   is:\n\n  \n    \n      \n        \n          \n            w\n          \n          \n            n\n            e\n            w\n          \n        \n        =\n        \n          \n            w\n          \n          \n            o\n            l\n            d\n          \n        \n        \u2212\n        \u03b7\n        E\n        \u2061\n        [\n        \n          z\n        \n        (\n        \n          \n            w\n          \n          \n            o\n            l\n            d\n          \n          \n            T\n          \n        \n        \n          z\n        \n        \n          )\n          \n            3\n          \n        \n        ]\n        .\n      \n    \n    {\\displaystyle \\mathbf {w} _{new}=\\mathbf {w} _{old}-\\eta \\operatorname {E} [\\mathbf {z} (\\mathbf {w} _{old}^{T}\\mathbf {z} )^{3}].}\n  where \n  \n    \n      \n        \u03b7\n      \n    \n    {\\displaystyle \\eta }\n   is a small constant to guarantee that \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n   converges to the optimal solution. After each update, we normalize \n  \n    \n      \n        \n          \n            w\n          \n          \n            n\n            e\n            w\n          \n        \n        =\n        \n          \n            \n              \n                w\n              \n              \n                n\n                e\n                w\n              \n            \n            \n              \n                |\n              \n              \n                \n                  w\n                \n                \n                  n\n                  e\n                  w\n                \n              \n              \n                |\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {w} _{new}={\\frac {\\mathbf {w} _{new}}{|\\mathbf {w} _{new}|}}}\n  , and set \n  \n    \n      \n        \n          \n            w\n          \n          \n            o\n            l\n            d\n          \n        \n        =\n        \n          \n            w\n          \n          \n            n\n            e\n            w\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {w} _{old}=\\mathbf {w} _{new}}\n  , and repeat the updating process until convergence. We can also use another algorithm to update the weight vector \n  \n    \n      \n        \n          w\n        \n      \n    \n    {\\displaystyle \\mathbf {w} }\n  .\nAnother approach is using negentropy instead of kurtosis. Using negentropy is a more robust method than kurtosis, as kurtosis is very sensitive to outliers. The negentropy methods are based on an important property of Gaussian distribution: a Gaussian variable has the largest entropy among all continuous random variables of equal variance. This is also the reason why  we want to find the most nongaussian variables. A simple proof can be found in Differential entropy.\n\n  \n    \n      \n        J\n        (\n        x\n        )\n        =\n        S\n        (\n        y\n        )\n        \u2212\n        S\n        (\n        x\n        )\n        \n      \n    \n    {\\displaystyle J(x)=S(y)-S(x)\\,}\n  y is a Gaussian random variable of the same covariance matrix as x\n\n  \n    \n      \n        S\n        (\n        x\n        )\n        =\n        \u2212\n        \u222b\n        \n          p\n          \n            x\n          \n        \n        (\n        u\n        )\n        log\n        \u2061\n        \n          p\n          \n            x\n          \n        \n        (\n        u\n        )\n        d\n        u\n      \n    \n    {\\displaystyle S(x)=-\\int p_{x}(u)\\log p_{x}(u)du}\n  An approximation for negentropy is\n\n  \n    \n      \n        J\n        (\n        x\n        )\n        =\n        \n          \n            1\n            12\n          \n        \n        (\n        E\n        (\n        \n          x\n          \n            3\n          \n        \n        )\n        \n          )\n          \n            2\n          \n        \n        +\n        \n          \n            1\n            48\n          \n        \n        (\n        k\n        u\n        r\n        t\n        (\n        x\n        )\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle J(x)={\\frac {1}{12}}(E(x^{3}))^{2}+{\\frac {1}{48}}(kurt(x))^{2}}\n  A proof can be found in the original papers of Comon; it has been reproduced in the book Independent Component Analysis by Aapo Hyv\xe4rinen, Juha Karhunen, and Erkki Oja This approximation also suffers from the same problem as kurtosis (sensitivity to outliers). Other approaches have been developed.\n\n  \n    \n      \n        J\n        (\n        y\n        )\n        =\n        \n          k\n          \n            1\n          \n        \n        (\n        E\n        (\n        \n          G\n          \n            1\n          \n        \n        (\n        y\n        )\n        )\n        \n          )\n          \n            2\n          \n        \n        +\n        \n          k\n          \n            2\n          \n        \n        (\n        E\n        (\n        \n          G\n          \n            2\n          \n        \n        (\n        y\n        )\n        )\n        \u2212\n        E\n        (\n        \n          G\n          \n            2\n          \n        \n        (\n        v\n        )\n        \n          )\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle J(y)=k_{1}(E(G_{1}(y)))^{2}+k_{2}(E(G_{2}(y))-E(G_{2}(v))^{2}}\n  A choice of \n  \n    \n      \n        \n          G\n          \n            1\n          \n        \n      \n    \n    {\\displaystyle G_{1}}\n   and \n  \n    \n      \n        \n          G\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle G_{2}}\n   are \n\n  \n    \n      \n        \n          G\n          \n            1\n          \n        \n        =\n        \n          \n            1\n            \n              a\n              \n                1\n              \n            \n          \n        \n        log\n        \u2061\n        (\n        cosh\n        \u2061\n        (\n        \n          a\n          \n            1\n          \n        \n        u\n        )\n        )\n      \n    \n    {\\displaystyle G_{1}={\\frac {1}{a_{1}}}\\log(\\cosh(a_{1}u))}\n   and \n  \n    \n      \n        \n          G\n          \n            2\n          \n        \n        =\n        \u2212\n        exp\n        \u2061\n        (\n        \u2212\n        \n          \n            \n              u\n              \n                2\n              \n            \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle G_{2}=-\\exp(-{\\frac {u^{2}}{2}})}\n  \n\n\n=== Based on infomax ===\nInfomax ICA is essentially a multivariate, parallel version of projection pursuit. Whereas projection pursuit extracts a series of signals one at a time from a set of M signal mixtures, ICA extracts M signals in parallel. This tends to make ICA more robust than projection pursuit.The projection pursuit method uses Gram-Schmidt orthogonalization to ensure the independence of the extracted signal, while ICA use infomax and maximum likelihood estimate to ensure the independence of the extracted signal. The Non-Normality of the extracted signal is achieved by assigning an appropriate model, or prior, for the signal.\nThe process of ICA based on infomax in short is: given a set of signal mixtures \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   and a set of identical independent model cumulative distribution functions(cdfs) \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n  , we seek the unmixing matrix \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n   which maximizes the joint entropy of the signals \n  \n    \n      \n        \n          Y\n        \n        =\n        g\n        (\n        \n          y\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {Y} =g(\\mathbf {y} )}\n  , where \n  \n    \n      \n        \n          y\n        \n        =\n        \n          W\n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {y} =\\mathbf {Wx} }\n   are the signals extracted by \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n  . Given the optimal \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n  , the signals \n  \n    \n      \n        \n          Y\n        \n      \n    \n    {\\displaystyle \\mathbf {Y} }\n   have maximum entropy and are therefore independent, which ensures that the extracted signals \n  \n    \n      \n        \n          y\n        \n        =\n        \n          g\n          \n            \u2212\n            1\n          \n        \n        (\n        \n          Y\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {y} =g^{-1}(\\mathbf {Y} )}\n   are also independent. \n  \n    \n      \n        g\n      \n    \n    {\\displaystyle g}\n   is an invertible function, and is the signal model. Note that if the source signal model probability density function \n  \n    \n      \n        \n          p\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle p_{s}}\n   matches the probability density function of the extracted signal \n  \n    \n      \n        \n          p\n          \n            \n              y\n            \n          \n        \n      \n    \n    {\\displaystyle p_{\\mathbf {y} }}\n  , then maximizing the joint entropy of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   also maximizes the amount of mutual information between \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n   and \n  \n    \n      \n        \n          Y\n        \n      \n    \n    {\\displaystyle \\mathbf {Y} }\n  . For this reason, using entropy to extract independent signals is known as infomax.\nConsider the entropy of the vector variable \n  \n    \n      \n        \n          Y\n        \n        =\n        g\n        (\n        \n          y\n        \n        )\n      \n    \n    {\\displaystyle \\mathbf {Y} =g(\\mathbf {y} )}\n  , where \n  \n    \n      \n        \n          y\n        \n        =\n        \n          W\n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {y} =\\mathbf {Wx} }\n   is the set of signals extracted by the unmixing matrix \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n  . For a finite set of values sampled from a distribution with pdf \n  \n    \n      \n        \n          p\n          \n            \n              y\n            \n          \n        \n      \n    \n    {\\displaystyle p_{\\mathbf {y} }}\n  , the entropy of \n  \n    \n      \n        \n          Y\n        \n      \n    \n    {\\displaystyle \\mathbf {Y} }\n   can be estimated as:\n\n  \n    \n      \n        H\n        (\n        \n          Y\n        \n        )\n        =\n        \u2212\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            t\n            =\n            1\n          \n          \n            N\n          \n        \n        ln\n        \u2061\n        \n          p\n          \n            \n              Y\n            \n          \n        \n        (\n        \n          \n            Y\n          \n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle H(\\mathbf {Y} )=-{\\frac {1}{N}}\\sum _{t=1}^{N}\\ln p_{\\mathbf {Y} }(\\mathbf {Y} ^{t})}\n  The joint pdf \n  \n    \n      \n        \n          p\n          \n            \n              Y\n            \n          \n        \n      \n    \n    {\\displaystyle p_{\\mathbf {Y} }}\n   can be shown to be related to the joint pdf \n  \n    \n      \n        \n          p\n          \n            \n              y\n            \n          \n        \n      \n    \n    {\\displaystyle p_{\\mathbf {y} }}\n   of the extracted signals by the multivariate form:\n\n  \n    \n      \n        \n          p\n          \n            \n              Y\n            \n          \n        \n        (\n        Y\n        )\n        =\n        \n          \n            \n              \n                p\n                \n                  \n                    y\n                  \n                \n              \n              (\n              \n                y\n              \n              )\n            \n            \n              \n                |\n              \n              \n                \n                  \n                    \u2202\n                    \n                      Y\n                    \n                  \n                  \n                    \u2202\n                    \n                      y\n                    \n                  \n                \n              \n              \n                |\n              \n            \n          \n        \n      \n    \n    {\\displaystyle p_{\\mathbf {Y} }(Y)={\\frac {p_{\\mathbf {y} }(\\mathbf {y} )}{|{\\frac {\\partial \\mathbf {Y} }{\\partial \\mathbf {y} }}|}}}\n  where \n  \n    \n      \n        \n          J\n        \n        =\n        \n          \n            \n              \u2202\n              \n                Y\n              \n            \n            \n              \u2202\n              \n                y\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\mathbf {J} ={\\frac {\\partial \\mathbf {Y} }{\\partial \\mathbf {y} }}}\n   is the Jacobian matrix. We have \n  \n    \n      \n        \n          |\n        \n        \n          J\n        \n        \n          |\n        \n        =\n        \n          g\n          \u2032\n        \n        (\n        \n          y\n        \n        )\n      \n    \n    {\\displaystyle |\\mathbf {J} |=g'(\\mathbf {y} )}\n  , and \n  \n    \n      \n        \n          g\n          \u2032\n        \n      \n    \n    {\\displaystyle g'}\n   is the pdf assumed for source signals \n  \n    \n      \n        \n          g\n          \u2032\n        \n        =\n        \n          p\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle g'=p_{s}}\n  , therefore,\n\n  \n    \n      \n        \n          p\n          \n            \n              Y\n            \n          \n        \n        (\n        Y\n        )\n        =\n        \n          \n            \n              \n                p\n                \n                  \n                    y\n                  \n                \n              \n              (\n              \n                y\n              \n              )\n            \n            \n              \n                |\n              \n              \n                \n                  \n                    \u2202\n                    \n                      Y\n                    \n                  \n                  \n                    \u2202\n                    \n                      y\n                    \n                  \n                \n              \n              \n                |\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                p\n                \n                  \n                    y\n                  \n                \n              \n              (\n              \n                y\n              \n              )\n            \n            \n              \n                p\n                \n                  \n                    s\n                  \n                \n              \n              (\n              \n                y\n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle p_{\\mathbf {Y} }(Y)={\\frac {p_{\\mathbf {y} }(\\mathbf {y} )}{|{\\frac {\\partial \\mathbf {Y} }{\\partial \\mathbf {y} }}|}}={\\frac {p_{\\mathbf {y} }(\\mathbf {y} )}{p_{\\mathbf {s} }(\\mathbf {y} )}}}\n  therefore,\n\n  \n    \n      \n        H\n        (\n        \n          Y\n        \n        )\n        =\n        \u2212\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            t\n            =\n            1\n          \n          \n            N\n          \n        \n        ln\n        \u2061\n        \n          \n            \n              \n                p\n                \n                  \n                    y\n                  \n                \n              \n              (\n              \n                y\n              \n              )\n            \n            \n              \n                p\n                \n                  \n                    s\n                  \n                \n              \n              (\n              \n                y\n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle H(\\mathbf {Y} )=-{\\frac {1}{N}}\\sum _{t=1}^{N}\\ln {\\frac {p_{\\mathbf {y} }(\\mathbf {y} )}{p_{\\mathbf {s} }(\\mathbf {y} )}}}\n  We know that when \n  \n    \n      \n        \n          p\n          \n            \n              y\n            \n          \n        \n        =\n        \n          p\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle p_{\\mathbf {y} }=p_{s}}\n  , \n  \n    \n      \n        \n          p\n          \n            \n              Y\n            \n          \n        \n      \n    \n    {\\displaystyle p_{\\mathbf {Y} }}\n   is of uniform distribution, and \n  \n    \n      \n        H\n        (\n        \n          \n            Y\n          \n        \n        )\n      \n    \n    {\\displaystyle H({\\mathbf {Y} })}\n   is maximized. Since\n\n  \n    \n      \n        \n          p\n          \n            \n              y\n            \n          \n        \n        (\n        \n          y\n        \n        )\n        =\n        \n          \n            \n              \n                p\n                \n                  \n                    x\n                  \n                \n              \n              (\n              \n                x\n              \n              )\n            \n            \n              \n                |\n              \n              \n                \n                  \n                    \u2202\n                    \n                      y\n                    \n                  \n                  \n                    \u2202\n                    \n                      x\n                    \n                  \n                \n              \n              \n                |\n              \n            \n          \n        \n        =\n        \n          \n            \n              \n                p\n                \n                  \n                    x\n                  \n                \n              \n              (\n              \n                x\n              \n              )\n            \n            \n              \n                |\n              \n              \n                W\n              \n              \n                |\n              \n            \n          \n        \n      \n    \n    {\\displaystyle p_{\\mathbf {y} }(\\mathbf {y} )={\\frac {p_{\\mathbf {x} }(\\mathbf {x} )}{|{\\frac {\\partial \\mathbf {y} }{\\partial \\mathbf {x} }}|}}={\\frac {p_{\\mathbf {x} }(\\mathbf {x} )}{|\\mathbf {W} |}}}\n  where \n  \n    \n      \n        \n          |\n        \n        \n          W\n        \n        \n          |\n        \n      \n    \n    {\\displaystyle |\\mathbf {W} |}\n   is the absolute value of the determinant of the unmixing matix \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n  . Therefore,\n\n  \n    \n      \n        H\n        (\n        \n          Y\n        \n        )\n        =\n        \u2212\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            t\n            =\n            1\n          \n          \n            N\n          \n        \n        ln\n        \u2061\n        \n          \n            \n              \n                p\n                \n                  \n                    x\n                  \n                \n              \n              (\n              \n                \n                  x\n                \n                \n                  t\n                \n              \n              )\n            \n            \n              \n                |\n              \n              \n                W\n              \n              \n                |\n              \n              \n                p\n                \n                  \n                    s\n                  \n                \n              \n              (\n              \n                \n                  y\n                \n                \n                  t\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle H(\\mathbf {Y} )=-{\\frac {1}{N}}\\sum _{t=1}^{N}\\ln {\\frac {p_{\\mathbf {x} }(\\mathbf {x} ^{t})}{|\\mathbf {W} |p_{\\mathbf {s} }(\\mathbf {y} ^{t})}}}\n  so,\n\n  \n    \n      \n        H\n        (\n        \n          Y\n        \n        )\n        =\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            t\n            =\n            1\n          \n          \n            N\n          \n        \n        ln\n        \u2061\n        \n          p\n          \n            \n              s\n            \n          \n        \n        (\n        \n          \n            y\n          \n          \n            t\n          \n        \n        )\n        +\n        ln\n        \u2061\n        \n          |\n        \n        \n          W\n        \n        \n          |\n        \n        +\n        H\n        (\n        \n          x\n        \n        )\n      \n    \n    {\\displaystyle H(\\mathbf {Y} )={\\frac {1}{N}}\\sum _{t=1}^{N}\\ln p_{\\mathbf {s} }(\\mathbf {y} ^{t})+\\ln |\\mathbf {W} |+H(\\mathbf {x} )}\n  since \n  \n    \n      \n        H\n        (\n        \n          x\n        \n        )\n        =\n        \u2212\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            t\n            =\n            1\n          \n          \n            N\n          \n        \n        ln\n        \u2061\n        \n          p\n          \n            \n              x\n            \n          \n        \n        (\n        \n          \n            x\n          \n          \n            t\n          \n        \n        )\n      \n    \n    {\\displaystyle H(\\mathbf {x} )=-{\\frac {1}{N}}\\sum _{t=1}^{N}\\ln p_{\\mathbf {x} }(\\mathbf {x} ^{t})}\n  , and maximizing \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n   does not affect \n  \n    \n      \n        \n          H\n          \n            \n              x\n            \n          \n        \n      \n    \n    {\\displaystyle H_{\\mathbf {x} }}\n  , so we can maximize the function\n\n  \n    \n      \n        h\n        (\n        \n          Y\n        \n        )\n        =\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            t\n            =\n            1\n          \n          \n            N\n          \n        \n        ln\n        \u2061\n        \n          p\n          \n            \n              s\n            \n          \n        \n        (\n        \n          \n            y\n          \n          \n            t\n          \n        \n        )\n        +\n        ln\n        \u2061\n        \n          |\n        \n        \n          W\n        \n        \n          |\n        \n      \n    \n    {\\displaystyle h(\\mathbf {Y} )={\\frac {1}{N}}\\sum _{t=1}^{N}\\ln p_{\\mathbf {s} }(\\mathbf {y} ^{t})+\\ln |\\mathbf {W} |}\n  to achieve the independence of extracted signal.\nIf there are M marginal pdfs of the model joint pdf \n  \n    \n      \n        \n          p\n          \n            \n              s\n            \n          \n        \n      \n    \n    {\\displaystyle p_{\\mathbf {s} }}\n   are independent and use the commonly super-gaussian model pdf for the source signals \n  \n    \n      \n        \n          p\n          \n            \n              s\n            \n          \n        \n        =\n        (\n        1\n        \u2212\n        tanh\n        \u2061\n        (\n        \n          s\n        \n        \n          )\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle p_{\\mathbf {s} }=(1-\\tanh(\\mathbf {s} )^{2})}\n  , then we have\n\n  \n    \n      \n        h\n        (\n        \n          Y\n        \n        )\n        =\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            M\n          \n        \n        \n          \u2211\n          \n            t\n            =\n            1\n          \n          \n            N\n          \n        \n        ln\n        \u2061\n        (\n        1\n        \u2212\n        tanh\n        \u2061\n        (\n        \n          \n            w\n            \n              i\n            \n            \n              T\n            \n          \n          \n            x\n            \n              t\n            \n          \n        \n        \n          )\n          \n            2\n          \n        \n        )\n        +\n        ln\n        \u2061\n        \n          |\n        \n        \n          W\n        \n        \n          |\n        \n      \n    \n    {\\displaystyle h(\\mathbf {Y} )={\\frac {1}{N}}\\sum _{i=1}^{M}\\sum _{t=1}^{N}\\ln(1-\\tanh(\\mathbf {w_{i}^{T}x^{t}} )^{2})+\\ln |\\mathbf {W} |}\n  In the sum, given an observed signal mixture  \n  \n    \n      \n        \n          x\n        \n      \n    \n    {\\displaystyle \\mathbf {x} }\n  , the corresponding set of extracted signals  \n  \n    \n      \n        \n          y\n        \n      \n    \n    {\\displaystyle \\mathbf {y} }\n    and source signal model \n  \n    \n      \n        \n          p\n          \n            \n              s\n            \n          \n        \n        =\n        \n          g\n          \u2032\n        \n      \n    \n    {\\displaystyle p_{\\mathbf {s} }=g'}\n  ,  we can find the optimal unmixing matrix \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n  , and make the extracted signals independent and non-gaussian. Like the projection pursuit situation, we can use gradient descent method to find the optimal solution of the unmixing matrix.\n\n\n=== Based on maximum likelihood estimation ===\nMaximum likelihood estimation (MLE) is a standard statistical tool for finding parameter values (e.g. the unmixing matrix \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n  ) that provide the best fit of some data (e.g., the extracted signals \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n  ) to a given a model (e.g., the assumed joint probability density function (pdf) \n  \n    \n      \n        \n          p\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle p_{s}}\n   of source signals).The ML "model" includes a specification of a pdf, which in this case is the pdf \n  \n    \n      \n        \n          p\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle p_{s}}\n   of the unknown source signals \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  . Using ML ICA, the objective is to find an unmixing matrix that yields extracted signals \n  \n    \n      \n        y\n        =\n        \n          W\n        \n        x\n      \n    \n    {\\displaystyle y=\\mathbf {W} x}\n   with a joint pdf as similar as possible to the joint pdf \n  \n    \n      \n        \n          p\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle p_{s}}\n   of the unknown source signals \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  .\nMLE is thus based on the assumption that if the model pdf \n  \n    \n      \n        \n          p\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle p_{s}}\n   and the model parameters \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n   are correct then a high probability should be obtained for the data \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   that were actually observed. Conversely, if \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n   is far from the correct parameter values then a low probability of the observed data would be expected.\nUsing MLE, we call the probability of the observed data for a given set of model parameter values (e.g., a pdf \n  \n    \n      \n        \n          p\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle p_{s}}\n   and a matrix \n  \n    \n      \n        \n          A\n        \n      \n    \n    {\\displaystyle \\mathbf {A} }\n  ) the likelihood of the model parameter values given the observed data.\nWe define a likelihood function \n  \n    \n      \n        \n          L\n          (\n          W\n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {L(W)} }\n   of \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n  :\n\n  \n    \n      \n        \n          L\n          (\n          W\n          )\n        \n        =\n        \n          p\n          \n            s\n          \n        \n        (\n        \n          W\n        \n        x\n        )\n        \n          |\n        \n        det\n        \n          W\n        \n        \n          |\n        \n        .\n      \n    \n    {\\displaystyle \\mathbf {L(W)} =p_{s}(\\mathbf {W} x)|\\det \\mathbf {W} |.}\n  \nThis equals to the probability density at \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , since \n  \n    \n      \n        s\n        =\n        \n          W\n        \n        x\n      \n    \n    {\\displaystyle s=\\mathbf {W} x}\n  .\nThus, if we wish to find a \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n   that is most likely to have generated the observed mixtures \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   from the unknown source signals \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n   with pdf \n  \n    \n      \n        \n          p\n          \n            s\n          \n        \n      \n    \n    {\\displaystyle p_{s}}\n   then we need only find that \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n   which maximizes the likelihood \n  \n    \n      \n        \n          L\n          (\n          W\n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {L(W)} }\n  . The unmixing matrix that maximizes equation is known as the MLE of the optimal unmixing matrix.\nIt is common practice to use the log likelihood, because this is easier to evaluate. As the logarithm is a monotonic function, the \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n   that maximizes the function \n  \n    \n      \n        \n          L\n          (\n          W\n          )\n        \n      \n    \n    {\\displaystyle \\mathbf {L(W)} }\n   also maximizes its logarithm \n  \n    \n      \n        ln\n        \u2061\n        \n          L\n          (\n          W\n          )\n        \n      \n    \n    {\\displaystyle \\ln \\mathbf {L(W)} }\n  . This allows us to take the logarithm of equation above, which yields the log likelihood function\n\n  \n    \n      \n        ln\n        \u2061\n        \n          L\n          (\n          W\n          )\n        \n        =\n        \n          \u2211\n          \n            i\n          \n        \n        \n          \u2211\n          \n            t\n          \n        \n        ln\n        \u2061\n        \n          p\n          \n            s\n          \n        \n        (\n        \n          w\n          \n            i\n          \n          \n            T\n          \n        \n        \n          x\n          \n            t\n          \n        \n        )\n        +\n        N\n        ln\n        \u2061\n        \n          |\n        \n        det\n        \n          W\n        \n        \n          |\n        \n      \n    \n    {\\displaystyle \\ln \\mathbf {L(W)} =\\sum _{i}\\sum _{t}\\ln p_{s}(w_{i}^{T}x_{t})+N\\ln |\\det \\mathbf {W} |}\n  \nIf we substitute a commonly used high-Kurtosis model pdf for the source signals \n  \n    \n      \n        \n          p\n          \n            s\n          \n        \n        =\n        (\n        1\n        \u2212\n        tanh\n        \u2061\n        (\n        s\n        \n          )\n          \n            2\n          \n        \n        )\n      \n    \n    {\\displaystyle p_{s}=(1-\\tanh(s)^{2})}\n   then we have\n\n  \n    \n      \n        ln\n        \u2061\n        \n          L\n          (\n          W\n          )\n        \n        =\n        \n          \n            1\n            N\n          \n        \n        \n          \u2211\n          \n            i\n          \n          \n            M\n          \n        \n        \n          \u2211\n          \n            t\n          \n          \n            N\n          \n        \n        ln\n        \u2061\n        (\n        1\n        \u2212\n        tanh\n        \u2061\n        (\n        \n          w\n          \n            i\n          \n          \n            T\n          \n        \n        \n          x\n          \n            t\n          \n        \n        \n          )\n          \n            2\n          \n        \n        )\n        +\n        ln\n        \u2061\n        \n          |\n        \n        det\n        \n          W\n        \n        \n          |\n        \n      \n    \n    {\\displaystyle \\ln \\mathbf {L(W)} ={1 \\over N}\\sum _{i}^{M}\\sum _{t}^{N}\\ln(1-\\tanh(w_{i}^{T}x_{t})^{2})+\\ln |\\det \\mathbf {W} |}\n  \nThis matrix \n  \n    \n      \n        \n          W\n        \n      \n    \n    {\\displaystyle \\mathbf {W} }\n   that maximizes this function is the maximum likelihood estimation.\n\n\n== History and background ==\nThe early general framework for independent component analysis was introduced by Jeanny H\xe9rault and Bernard Ans from 1984, further developed by Christian Jutten in 1985 and 1986, and refined by Pierre Comon in 1991, and popularized in his paper of 1994. In 1995, Tony Bell and Terry Sejnowski introduced a fast and efficient ICA algorithm based on infomax, a principle introduced by Ralph Linsker in 1987.\nThere are many algorithms available in the literature which do ICA. A largely used one, including in industrial applications, is the FastICA algorithm, developed by Hyv\xe4rinen and Oja, which uses the kurtosis as cost function. Other examples are rather related to blind source separation where a more general approach is used. For example, one can drop the independence assumption and separate mutually correlated signals, thus, statistically "dependent" signals. Sepp Hochreiter and J\xfcrgen Schmidhuber showed how to obtain non-linear ICA or source separation as a by-product of regularization (1999). Their method does not require a priori knowledge about the number of independent sources.\n\n\n== Applications ==\nICA can be extended to analyze non-physical signals. For instance, ICA has been applied to discover discussion topics on a bag of news list archives.\nSome ICA applications are listed below:\n\noptical Imaging of neurons\nneuronal spike sorting\nface recognition\nmodelling receptive fields of primary visual neurons\npredicting stock market prices\nmobile phone communications \ncolour based detection of the ripeness of tomatoes\nremoving artifacts, such as eye blinks, from EEG data.\nanalysis of changes in gene expression over time in single cell RNA-sequencing experiments.\nstudies of the resting state network of the brain.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\nComon, Pierre (1994): "Independent Component Analysis: a new concept?", Signal Processing, 36(3):287\u2013314 (The original paper describing the concept of ICA)\nHyv\xe4rinen, A.; Karhunen, J.; Oja, E. (2001): Independent Component Analysis, New York: Wiley, ISBN 978-0-471-40540-5 ( Introductory chapter )\nHyv\xe4rinen, A.; Oja, E. (2000): "Independent Component Analysis: Algorithms and Application", Neural Networks, 13(4-5):411-430. (Technical but pedagogical introduction).\nComon, P.; Jutten C., (2010): Handbook of Blind Source Separation, Independent Component Analysis and Applications. Academic Press, Oxford UK. ISBN 978-0-12-374726-6\nLee, T.-W. (1998): Independent component analysis: Theory and applications, Boston, Mass: Kluwer Academic Publishers, ISBN 0-7923-8261-7\nAcharyya, Ranjan (2008): A New Approach for Blind Source Separation of Convolutive Sources - Wavelet Based Separation Using Shrinkage Function ISBN 3-639-07797-0 ISBN 978-3639077971 (this book focuses on unsupervised learning with Blind Source Separation)\n\n\n== External links ==\nWhat is independent component analysis? by Aapo Hyv\xe4rinen\nIndependent Component Analysis: A Tutorial by Aapo Hyv\xe4rinen\nA Tutorial on Independent Component Analysis\nFastICA as a package for Matlab, in R language, C++\nICALAB Toolboxes for Matlab, developed at RIKEN\nHigh Performance Signal Analysis Toolkit provides C++ implementations of FastICA and Infomax\nICA toolbox Matlab tools for ICA with Bell-Sejnowski, Molgedey-Schuster and mean field ICA. Developed at DTU.\nDemonstration of the cocktail party problem\nEEGLAB Toolbox ICA of EEG for Matlab, developed at UCSD.\nFMRLAB Toolbox ICA of fMRI for Matlab, developed at UCSD\nMELODIC, part of the FMRIB Software Library.\nDiscussion of ICA used in a biomedical shape-representation context\nFastICA, CuBICA, JADE and TDSEP algorithm for Python and more...\nGroup ICA Toolbox and Fusion ICA Toolbox\nTutorial: Using ICA for cleaning EEG signals