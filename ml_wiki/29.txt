In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.\nThe RVM has an identical functional form to the support vector machine, but provides probabilistic classification.\nIt is actually equivalent to a Gaussian process model with covariance function:\n\n  \n    \n      \n        k\n        (\n        \n          x\n        \n        ,\n        \n          \n            x\n            \u2032\n          \n        \n        )\n        =\n        \n          \u2211\n          \n            j\n            =\n            1\n          \n          \n            N\n          \n        \n        \n          \n            1\n            \n              \u03b1\n              \n                j\n              \n            \n          \n        \n        \u03c6\n        (\n        \n          x\n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n        \u03c6\n        (\n        \n          \n            x\n          \n          \u2032\n        \n        ,\n        \n          \n            x\n          \n          \n            j\n          \n        \n        )\n      \n    \n    {\\displaystyle k(\\mathbf {x} ,\\mathbf {x'} )=\\sum _{j=1}^{N}{\\frac {1}{\\alpha _{j}}}\\varphi (\\mathbf {x} ,\\mathbf {x} _{j})\\varphi (\\mathbf {x} ',\\mathbf {x} _{j})}\n  where \n  \n    \n      \n        \u03c6\n      \n    \n    {\\displaystyle \\varphi }\n   is the kernel function (usually Gaussian), \n  \n    \n      \n        \n          \u03b1\n          \n            j\n          \n        \n      \n    \n    {\\displaystyle \\alpha _{j}}\n   are the variances of the prior on the weight vector\n\n  \n    \n      \n        w\n        \u223c\n        N\n        (\n        0\n        ,\n        \n          \u03b1\n          \n            \u2212\n            1\n          \n        \n        I\n        )\n      \n    \n    {\\displaystyle w\\sim N(0,\\alpha ^{-1}I)}\n  , and \n  \n    \n      \n        \n          \n            x\n          \n          \n            1\n          \n        \n        ,\n        \u2026\n        ,\n        \n          \n            x\n          \n          \n            N\n          \n        \n      \n    \n    {\\displaystyle \\mathbf {x} _{1},\\ldots ,\\mathbf {x} _{N}}\n   are the input vectors of the training set.Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem).\nThe relevance vector machine is patented in the United States by Microsoft (patent expired September 4, 2019).\n\n\n== See also ==\nKernel trick\nPlatt scaling: turns an SVM into a probability model\n\n\n== References ==\n\n\n== Software ==\ndlib C++ Library\nThe Kernel-Machine Library\nrvmbinary: R package for binary classification\nscikit-rvm\nfast-scikit-rvm, rvm tutorial\n\n\n== External links ==\nTipping's webpage on Sparse Bayesian Models and the RVM\nA Tutorial on RVM by Tristan Fletcher\nApplied tutorial on RVM\nComparison of RVM and SVM